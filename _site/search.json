[
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "In the context of this project, unsupervised learning will help identify groups of food items with similar waste characteristics, such as nutritional composition, environmental impact, and economic loss. These clusters can provide actionable insights for targeting interventions, whether through donation strategies, better supply chain management, or more efficient waste disposal practices. Additionally, dimensionality reduction methods like PCA and t-SNE will be used to visualize the multidimensional nature of the food waste data, offering a more intuitive understanding of how different factors correlate and contribute to the problem. By applying unsupervised learning techniques to the Nutrient Waste dataset, this project aims to uncover novel insights that can inform more sustainable food management strategies and policies."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#what-to-address",
    "href": "technical-details/unsupervised-learning/main.html#what-to-address",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "In this analysis, I aim to explore the relationship between food waste, nutritional composition, and associated economic and environmental impacts. Specifically, I are building predictive models to address two key questions: (1) What factors contribute to food being classified as unfit for human consumption? and (2) How can I estimate the surplus value of food waste in monetary terms? By answering these questions, I aim to identify patterns and characteristics that can inform strategies for waste reduction and better resource allocation.\nThe analysis seeks to combine multiple domains, including nutrition, sustainability, and machine learning, to provide actionable insights. This work is motivated by the growing need to address food waste as a critical global challenge, with implications for food security, environmental sustainability, and economic efficiency."
  },
  {
    "objectID": "technical-details/eda/main.html#suggested-page-structure",
    "href": "technical-details/eda/main.html#suggested-page-structure",
    "title": "Exploratory Data Analysis",
    "section": "Suggested page structure",
    "text": "Suggested page structure\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/main.html#what-to-address",
    "href": "technical-details/eda/main.html#what-to-address",
    "title": "Exploratory Data Analysis",
    "section": "What to address",
    "text": "What to address\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "The data cleaning process for this project involved several key steps aimed at preparing the raw food waste and food data for analysis. Initially, we removed unnecessary columns and filtered the data based on relevant criteria, such as valid serving size units. We also handled missing data by dropping rows with missing values in critical fields, ensuring the integrity of the dataset.\nNext, we standardized measurement units by converting various food quantities to grams, ensuring consistency across the dataset. This was particularly important for nutritional analysis, where accurate comparisons are required. Additionally, we transformed specific columns (e.g., vitamins, cholesterol) into consistent units, further improving data uniformity.\nWe applied fuzzy matching techniques to merge food waste data with cleaned food data based on food name similarities. The fuzzy join method, using difflib’s SequenceMatcher, allowed us to handle slight variations in food names between datasets and find the best matches.\nThroughout the process, we focused on ensuring that the data was in a format suitable for analysis, with correctly formatted data types, consistent units, and minimal missing or inconsistent data. This rigorous cleaning approach enables more reliable exploratory data analysis (EDA) and model building, setting the foundation for insightful analysis of food waste and nutrition."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "title": "Data Cleaning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#general-comments",
    "href": "technical-details/data-cleaning/main.html#general-comments",
    "title": "Data Cleaning",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#what-to-address",
    "href": "technical-details/data-cleaning/main.html#what-to-address",
    "title": "Data Cleaning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "The prompts below were used to chat with GPT-4o and build this project."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\nhelp me build out a brief literature review section on this topic. relevant to our questions of classifying food donations based on the nutrients of food waste, and building a regression of dollar surplus and one for classifying the primary disposal method"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\nRephrase the following and make them better structured sentences with richer analytical content -\nThe above scatter plot with a trendline shows the clear positive relationship protein rich foods have with the gallons of water associated with their upstream manufacturing process.\nThe above elbow plot reveals that 3 clusters is the optimal number of clusters. Applying this to the PCA reduced data we see that this may be the correct option but there are points between cluster “0” and cluster “1” which may be ambiguous.\nThe above commented out code performs a grid search over values of epsilon and min-samples to find the optimal params which maximise the silhouette score. The optimal params are {‘eps’: 0.1, ‘min_samples’: 5}.\nDBSCAN has a silhouette score 0.5665 and several many points labelled as noise indicated by -1. This indicates that this clustering method does not work well"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\nhow do I include a cell_id from a ipynb\nhow do i get correlational coef in python for 2 df cols\nI got the following errors could you help me with what it means and why Im getting it -\nInput X contains NaN. KMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\np/anaconda3/lib/python3.12/site-packages/scipy/linalg/_misc.py:146, in norm(a, ord, axis, keepdims, check_finite) 144 # Differs from numpy only in non-finite handling and the use of blas. 145 if check_finite: –&gt; 146 a = np.asarray_chkfinite(a) 147 else: 148 a = np.asarray(a) … –&gt; 630 raise ValueError( 631 “array must not contain infs or NaNs”) 632 return a\nValueError: array must not contain infs or NaNs"
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Food waste is a global issue with profound economic, environmental, and social implications. According to the Food and Agriculture Organization (FAO), roughly one-third of all food produced globally—about 1.3 billion tons—is wasted annually, leading to significant losses in natural resources, energy, and human labor (FAO, 2013). In the United States, the Environmental Protection Agency (EPA) estimates that food waste constitutes 24% of municipal solid waste, making it the single largest category in landfills (EPA, 2021).\nThis analysis examines a comprehensive dataset capturing the surplus and waste of various food categories, along with their nutritional compositions. The goal is to uncover insights into:\n1.  The patterns and drivers of food surplus and waste.\n\n2.  The relationship between nutrient density and food waste across categories.\n\n3.  How resources like water and energy are impacted by food wastage.\nBy quantifying these metrics and exploring trends, this analysis aims to inform policies and interventions to mitigate food waste while addressing nutritional and environmental challenges."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you on this page will be specific you your project and data. Some things might “make more sense” on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Instructions",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses",
    "section": "",
    "text": "Food waste represents a critical issue at the intersection of sustainability, public health, and economics. Globally, one-third of all food produced is wasted, leading to significant environmental strain, economic loss, and missed opportunities to alleviate hunger. Addressing this challenge requires an interdisciplinary approach, leveraging data science to analyze and inform actionable solutions. My project, Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses, aims to bridge these dimensions and provide a comprehensive framework for understanding and mitigating food waste.\nThrough this project, I aim to classify food donations based on nutrient composition, predict dollar surplus associated with surplus food, and explore primary disposal methods using advanced statistical and machine learning techniques. These efforts will contribute to smarter policies and practices for a more sustainable future."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\n\n\n\n\n\n\nServing Size vs Tons of Uneaten Food\n\n\nTo begin the project, first read the instruction document (click here). This document is also accessible from the navigation bar.\nOnce you’ve completed that, you can proceed with the instructions found throughout the website."
  },
  {
    "objectID": "index.html#what-to-include-on-this-page",
    "href": "index.html#what-to-include-on-this-page",
    "title": "Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses",
    "section": "What to Include on This Page",
    "text": "What to Include on This Page\nThis is the landing page for your project. Content from this page can be reused in sections of your final report.\n\nAbout Me\n\n\n\nShanay Wadhwani\n\n\nHi, I’m Shanay Wadhwani, a data scientist and graduate student at Georgetown University, currently pursuing a Master of Science in Data Science and Analytics. My academic and professional journey is guided by a strong commitment to using data for meaningful societal change, particularly in sustainability and public health. This passion drives my current project, “Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses.”\nThis project focuses on exploring the multifaceted impact of food waste, delving into its economic costs, nutritional losses, and environmental consequences. By leveraging advanced data science techniques like Principal Component Analysis (PCA), clustering algorithms, and predictive modeling, I aim to classify food categories based on waste patterns and identify actionable insights to mitigate waste. My goal is to develop a comprehensive statistical interface that informs stakeholders about the ripple effects of food waste and highlights pathways for sustainability. Through this work, I hope to promote smarter policies and practices in food systems, from farm to table.\nAt Georgetown, I’ve cultivated a strong foundation in Natural Language Processing (NLP), probabilistic modeling, and computational linguistics, complemented by my leadership roles as a Graduate Student Senator and Student Ambassador. My role as a Teaching Assistant for the Data Mining course has deepened my ability to collaborate and mentor others, qualities I bring to every project I undertake.\nProfessionally, I’ve applied my expertise to diverse domains, including ETL development, AI-driven solutions, and financial modeling. At Argus Media, I spearheaded the development of ETL pipelines using the Golem framework, streamlining the processing of over a million pricing entries monthly while managing a team of 11 engineers. This experience honed my skills in project management and workflow optimization.\nMy technical toolkit includes Python, R, SQL, TensorFlow, Keras, ETL, and AWS, enabling me to solve complex problems in machine learning, deep learning, and statistical modeling. Whether it’s enhancing financial forecasting models at ClearPrism or exploring sustainability in my current research, I strive to combine technical precision with real-world impact.\nThe “Nutrient Waste” project reflects my broader mission to leverage data science for sustainable change. By analyzing the intersections of food waste, nutrition, and environmental resources, I aim to contribute to a more sustainable and equitable future. If you’re interested in this work or exploring potential collaborations, I’d love to connect. You can reach me at sw1449@georgetown.edu or explore my projects on my GitHub Profile. Let’s create solutions that matter!\n\n\nLiterature Review: Leveraging Data Science for Food Waste Analysis\nThe problem of food waste is a pressing global challenge, with significant environmental, economic, and social implications. Research in this domain increasingly focuses on utilizing data science techniques to uncover patterns, predict outcomes, and inform decision-making processes. This literature review synthesizes key studies relevant to our objectives: classifying food donations based on nutrient content, building a regression model for dollar surplus, and predicting primary disposal methods.\n\n\n\nFood Waste and Nutrient Analysis\nFood waste analysis often incorporates nutritional dimensions to understand its broader implications. Researchers like Beretta et al. (2013) highlight the nutritional losses associated with food waste, emphasizing the need to connect food surplus with nutrient availability for donation purposes. Studies leveraging machine learning, such as those by Fiore et al. (2020), demonstrate the utility of classification algorithms in categorizing waste by food type and nutrient content. These approaches can guide interventions to redistribute surplus food to populations in need while minimizing waste.\nEconomic Valuation of Food Surplus\nEconomic studies, such as those by Gunders et al. (2017), have assessed the dollar value of food surplus and waste, shedding light on the financial implications of inefficiencies in the food system. Regression models are frequently applied to estimate surplus value, considering variables like production costs, market value, and transportation. These models provide actionable insights for optimizing redistribution strategies and reducing financial losses.\nDisposal Method Classification\nIdentifying the primary disposal method for food waste is critical for environmental sustainability. Studies like those by Papargyropoulou et al. (2014) have categorized disposal methods—such as landfill, composting, and anaerobic digestion—based on factors like waste type, volume, and location. Machine learning classification models, as demonstrated by Song et al. (2018), are particularly effective in predicting disposal methods, combining categorical features with waste composition data.\nData Science Applications in Food Systems\nRecent advancements in data science tools and techniques have significantly impacted food waste research. Unsupervised learning methods, including Principal Component Analysis (PCA) and clustering algorithms, have been employed to uncover patterns in food waste characteristics (e.g., nutrient composition and disposal methods). Supervised learning, including regression and classification, has been applied to forecast outcomes like economic surplus and redistribution potential.\nGaps and Opportunities\nWhile substantial progress has been made, there are gaps in integrating nutritional, economic, and environmental dimensions into a unified analytical framework. Few studies explicitly combine nutrient profiling with economic surplus modeling or disposal classification. Bridging these dimensions could yield a comprehensive approach to managing food waste more effectively. Additionally, the inclusion of socio-economic factors, such as accessibility and donation feasibility, remains underexplored.\nRelevance to Current Research\nThis body of work informs our project’s design in the following ways: 1. Classification of Food Donations: Drawing on techniques from Fiore et al. (2020), we aim to build nutrient-based classification models to optimize food redistribution. 2. Regression Modeling for Dollar Surplus: Guided by Gunders et al. (2017), we will leverage regression analysis to quantify the economic value of surplus food. 3. Disposal Method Prediction: Inspired by Song et al. (2018), we intend to develop a predictive model for disposal methods, incorporating both categorical and continuous data.\nMy project seeks to advance the integration of these dimensions, addressing gaps identified in the literature and contributing to a holistic understanding of food waste management."
  },
  {
    "objectID": "index.html#additional-ideas-for-things-to-include",
    "href": "index.html#additional-ideas-for-things-to-include",
    "title": "Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses",
    "section": "Additional Ideas for things to include",
    "text": "Additional Ideas for things to include\n\nAudience: Who is this for? Data professionals, businesses, researchers, or curious readers.\nHeadline: A captivating title introducing the data science theme (e.g., “Unlocking Insights Through Data Stories”).\nIntroduction: A brief, engaging overview of what the website offers (e.g., data-driven stories, insights, or case studies).\nQuestions You Are Addressing: What do you hope to learn?\nMotivation: Explain why this topic matters, highlighting the importance of data in solving real-world problems.\nKey Topics: List the main focus areas (e.g., machine learning, data visualization, predictive modeling).\nUse Cases/Examples: A brief teaser of compelling stories or case studies you’ve worked on.\nCall to Action: Invite visitors to explore the content, follow along, or contact you for more information.\nVisual/Infographic: Add a simple graphic or visual element to make the page more dynamic."
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Final Report",
    "section": "",
    "text": "This report focuses on analyzing patterns of food waste and employs a data-driven approach to address two key objectives: reducing food wastage and minimizing surplus costs in US dollars. By leveraging insights into the macronutrient profiles of donated and recycled foods, the project aims to better understand the composition and potential uses of discarded food.\nTo achieve these goals, machine learning models are developed to predict the journey of food waste, providing actionable insights into its lifecycle. These models help identify opportunities to redirect surplus food toward donations or recycling pathways, rather than contributing to waste. The project also seeks to design and optimize new pipelines to mitigate food waste, transforming discarded nutrients into valuable resources that can benefit both communities and the environment.\n\n\n\nThis project seeks to analyze patterns in food waste with the goal of reducing wastage by increasing food donations and recycling opportunities. By examining food wastage trends over time, the project aims to design a nutrient reuptake pipeline, repurposing discarded nutrients for better use. Through a data-driven approach, the study investigates the journey of food waste, classifying its final destination—such as composting, animal feed, or donations—based on nutrient profiles.\nAdditionally, the project leverages machine learning models to predict the US Dollar surplus value using features derived from upstream processes. This predictive capability not only highlights areas of inefficiency but also provides actionable insights to reduce food production surpluses, optimize resource allocation, and align production more closely with demand, contributing to sustainability and cost reduction efforts.\n\n\n\nThe key findings of this study are presented below -\n\nUsing input features such as ‘gallons_water_footprint,’ ‘serving_size,’ ‘calories,’ ‘protein,’ ‘fat,’ ‘carbs,’ ‘fiber,’ ‘calcium,’ ‘iron,’ and ‘sodium,’ our Random Forest model achieves an impressive classification accuracy of 93%. This model identifies the primary disposal method for various food items, providing actionable insights into food waste management. The high accuracy of this model offers a practical tool for consumers, helping them determine which foods are suitable for recycling as compost or animal feed. By promoting informed disposal decisions, this approach can contribute to reducing overall food waste and supporting ssustainable waste management practices.\nBased on the aforementioned input features our KNN model can classify a food item as either “donatable” or not with a cross validation score of approximately 94%. Integrating this model into the existing logistics pipeline for food waste could help boost food donation efforts and spearhead more sustainable food management practices.\nUsing input features such as ‘serving_size,’ ‘calories,’ ‘protein,’ ‘fat,’ ‘carbs,’ ‘fiber,’ ‘calcium,’ ‘iron,’ ‘sodium,’ and ‘food_category’ (one-hot encoded), our Random Forest model achieved an R-squared value of approximately 0.968 in predicting the US Dollar Surplus value. This high R-squared value indicates that the model can accurately explain nearly 97% of the variability in surplus spending based on these input features.\n\n\n\n\nThis analysis employed a combination of exploratory data analysis (EDA), normalization, and statistical modeling techniques to draw insights from the data. The following EDA techniques were used -\n• The data was cleaned so that missing values were handled, and nutrient columns were standardized and converted to their appropriate units (e.g., grams or milligrams per serving), and addressing outliers.\n• Visualizations were prepared to explore the relationships between the variables in the data.\nNutritional data for protein, fat, carbs, fiber, vitamins, and minerals were often expressed in inconsistent units across the dataset (e.g., grams for macronutrients and milligrams for micronutrients). Standardizing these units enabled meaningful comparisons across categories. For instance, macronutrients (protein, fat, carbs) were standardized to grams per serving.\nThen, the following supervised and unsupervised learning techniques were applied to the dataset -\n• Supervised Learning models like KNN, SVM and Randomforest were used for multiclass classification problems. These techniques were used to predict the primary disposal method for a certain food item and to predict whether a food item could be donated or not\n• Regression models were used to explore relationships between waste generation and external factors like supply and surplus.\n• Techniques like K-Means were applied to group food categories based on nutritional similarity, highlighting waste trends in nutrient-dense versus nutrient-poor foods.\nThese methods provided a robust framework for understanding food waste and its nutritional implications while offering actionable insights.\n\n\n\nThis section provides a few important data visualizations along with drawn from them.\n\n\n\nServing Size (g) vs Tons Uneaten\n\n\nThe above plot helps visualize how serving sizes of food items (in grams) is positively correlated with tons of food uneaten. A major finding of this study is that smaller serving sizes can prevent food from being wasted. Furthermore, smaller serving sizes would also reduce the cost of production for companies in the long term.\n\n\n\nProtein Content (g) vs USD Surplus\n\n\nThe above plot indicates that high-protein foods are frequently associated with higher USD surplus values. This insight could be valuable in educating individuals and families about food consumption habits. Foods with high protein content disproportionately contribute to higher USD surplus values, highlighting their significant role in food waste and surplus production.\nTo mitigate this, consumers can be encouraged to plan their purchases more effectively, avoiding over-purchasing high-protein foods that are more likely to go unused. Additionally, this finding underscores the importance of targeted interventions in food donation or recycling programs, focusing on redistributing surplus high-protein foods to areas where they can be utilized, such as food banks or animal feed initiatives. By addressing the surplus generation of such nutrient-dense foods, efforts to reduce food waste can become more impactful and sustainable.\n\n\n\nProtein Content (g) vs Gallons of Water Footprint\n\n\nThe above plot demonstrates that foods with higher protein content (measured in grams) require significantly more gallons of water during upstream processing. This trend is likely attributable to high-protein foods such as beef, poultry, and dairy, which have a substantial environmental footprint due to intensive resource requirements in their production processes.\nThis finding emphasizes the need for promoting more sustainable dietary choices. Reducing the consumption of resource-intensive, high-protein foods can help minimize water usage and decrease the overall environmental impact of food production. Public awareness campaigns and policy interventions could encourage shifts toward plant-based proteins and other sustainable alternatives, fostering a more environmentally conscious approach to food consumption.\n\n\n\nFiber Content (g) vs Tons Recycled\n\n\nThe above plot illustrates that foods with a high fiber content, often less processed and more natural, are frequently recyclable and repurposable. This finding highlights the sustainability of consuming such foods, as they are less likely to contribute to waste and can instead serve as valuable inputs for recycling initiatives, such as composting or creating animal feed.\nEncouraging the consumption of high-fiber foods not only promotes healthier dietary habits but also aligns with sustainable food systems by reducing landfill contributions and supporting nutrient cycling. Public awareness campaigns and educational initiatives could emphasize the dual benefits of high-fiber foods—both for individual health and for reducing the environmental impact of food waste\n\n\n\nKernel Density Plot of Serving Size (g) vs USD Surplus\n\n\nThe above Kernel Density Plot highlights that foods with serving sizes between 30-80 grams or 150-200 grams contribute significantly to higher USD surplus values, indicating that these portions are more prone to being wasted. This insight underscores the importance of rethinking packaging strategies for these specific serving sizes.\nAdjusting packaging to better match consumption patterns—such as offering smaller, customizable portions or resealable options—could help mitigate waste. By aligning serving sizes more closely with consumer needs, the food industry can reduce excess production and waste, ultimately leading to both economic and environmental benefits.\n\n\n\nThe implications of the finding from this report can be found below.\n\nAnticipating Excess Production Costs:\n\nThe models presented here, provide the food and beverage industry with a predictive tool to estimate surplus spending. By identifying patterns linked to food overproduction, food and beverage businesses can better plan production quantities, aligning supply more closely with demand.\n\nReducing Food Waste at the Source:\n\nBy forecasting surplus values, companies and individual consumers can take preemptive measures to reduce production of items prone to excessive waste. This can lead to a significant reduction in food surpluses, contributing to more sustainable food systems.\n\nCost Optimization:\n\nManufacturers and suppliers can leverage these predictions to reduce operational costs associated with surplus management, such as storage, distribution, or disposal expenses. This ultimately enhances profitability and operational efficiency.\n\nSupport for Sustainability Goals:\n\nWith actionable insights from the model, the food industry can make more environmentally responsible decisions by minimizing its water footprint stemming from surplus production and waste.\n\n\n\nHere are some of the recommendations from this study -\n\nOptimize food packaging - The packaging for foods with a serving size between 30-80 grams or 150-200 grams must be redesigned because they currently contribute disproportionately to USD surplus values. Resealable and portion-controlled packaging can help reduce waste from food.\nSupport for Recycling and Food Donation Infrastructure - Policy makers should invest in infrastructure for food recycling such as anaerobic digestion farms or composting units. Furthermore, there should be tax benefits for donating surplus food and a wider encouragement for food recovery iniatives.\nUtilizing Leftovers and Expired Foods - Individuals can incorporate leftover ingredients creatively to reduce waste and maximize food utilization.\nEmbracing High-Fiber Foods - Individual consumers should consider consuming foods with a high fiber content. Food waste which comes from fiber-rich foods should be composted and/or recycled.\nFurther Investigation into Nutrient Reuptake Systems - While this is a promising study, further research is required into the nutrient needs of different organisms so that discarded nutrients can be used for not just animal feed and anaerobic digestion but also as fodder for other organisms.\n\n\n\n\nIn conclusion, this study is a promising step toward understanding and mitigating food waste by leveraging data-driven approaches. By analyzing the nutrient profiles and waste patterns of food, we have identified key factors that contribute to food surpluses and waste, such as serving size, protein content, and water footprint. The use of machine learning models has shown potential for predicting surplus values and identifying opportunities for food donations or recycling, thereby helping reduce food waste and its environmental impact. The recommendations provided can guide the food and beverage industry, policymakers, and consumers toward more sustainable practices, ultimately contributing to a more efficient and less wasteful food system. However, further research and continuous improvements in data collection and model accuracy will be essential to fully optimize these strategies for wider adoption."
  },
  {
    "objectID": "report/report.html#guidelines-for-creating-a-good-narrative",
    "href": "report/report.html#guidelines-for-creating-a-good-narrative",
    "title": "Final Report",
    "section": "",
    "text": "Clear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content."
  },
  {
    "objectID": "report/report.html#report-content",
    "href": "report/report.html#report-content",
    "title": "Final Report",
    "section": "",
    "text": "These are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights."
  },
  {
    "objectID": "report/report.html#final-tips",
    "href": "report/report.html#final-tips",
    "title": "Final Report",
    "section": "",
    "text": "Simplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "During the course of this project, several technical challenges were encountered that impacted the analysis and interpretation of food waste data. One of the primary obstacles was the complexity and volume of the dataset, which included a wide range of food-related attributes, such as nutritional information, environmental impact, and waste characteristics. Handling missing data, inconsistent formatting, and discrepancies in unit measurements posed significant challenges during the preprocessing phase. Despite employing various data-cleaning techniques, some portions of the dataset still contained gaps or outliers that required careful treatment to avoid skewing the results.\nComputational limitations also became apparent when applying more complex machine learning algorithms, such as deep learning models or large-scale clustering techniques. These models required significant memory and processing power, particularly during the dimensionality reduction and clustering steps. Additionally, some algorithms, such as DBSCAN and hierarchical clustering, were sensitive to parameter tuning and often led to unexpected results, such as a high number of noise points or unclear cluster boundaries. This underlined the importance of selecting appropriate algorithms and fine-tuning their parameters for optimal results.\nUnexpected results, particularly in clustering and classification tasks, were observed when trying to identify distinct patterns in the food waste data. For instance, while PCA and t-SNE provided valuable insights into the structure of the data, they sometimes revealed overlapping clusters, making it difficult to draw definitive conclusions. These findings highlighted the inherent complexity of food waste patterns and suggested that more sophisticated feature engineering or additional data sources might be needed to improve model accuracy.\nFuture work should focus on refining the feature engineering process to account for the broader contextual factors surrounding food waste, such as geographical location, socioeconomic status, and consumer behavior. Additionally, improving data quality through better standardization and exploring alternative machine learning models, such as ensemble techniques or advanced deep learning models, could enhance performance. Scaling the solution to handle larger datasets with more granular information would also be an important next step."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "During the course of this project, several technical challenges were encountered that impacted the analysis and interpretation of food waste data. One of the primary obstacles was the complexity and volume of the dataset, which included a wide range of food-related attributes, such as nutritional information, environmental impact, and waste characteristics. Handling missing data, inconsistent formatting, and discrepancies in unit measurements posed significant challenges during the preprocessing phase. Despite employing various data-cleaning techniques, some portions of the dataset still contained gaps or outliers that required careful treatment to avoid skewing the results.\nComputational limitations also became apparent when applying more complex machine learning algorithms, such as deep learning models or large-scale clustering techniques. These models required significant memory and processing power, particularly during the dimensionality reduction and clustering steps. Additionally, some algorithms, such as DBSCAN and hierarchical clustering, were sensitive to parameter tuning and often led to unexpected results, such as a high number of noise points or unclear cluster boundaries. This underlined the importance of selecting appropriate algorithms and fine-tuning their parameters for optimal results.\nUnexpected results, particularly in clustering and classification tasks, were observed when trying to identify distinct patterns in the food waste data. For instance, while PCA and t-SNE provided valuable insights into the structure of the data, they sometimes revealed overlapping clusters, making it difficult to draw definitive conclusions. These findings highlighted the inherent complexity of food waste patterns and suggested that more sophisticated feature engineering or additional data sources might be needed to improve model accuracy.\nFuture work should focus on refining the feature engineering process to account for the broader contextual factors surrounding food waste, such as geographical location, socioeconomic status, and consumer behavior. Additionally, improving data quality through better standardization and exploring alternative machine learning models, such as ensemble techniques or advanced deep learning models, could enhance performance. Scaling the solution to handle larger datasets with more granular information would also be an important next step."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "DSAN-5000: Project",
    "section": "Benchmarks",
    "text": "Benchmarks\nThe results of this project were compared to relevant industry benchmarks and existing research on food waste management and prediction. In general, the findings aligned with expectations from similar research, which suggests that food waste follows complex patterns influenced by various factors, such as nutritional content, environmental footprint, and supply chain dynamics.\nIn terms of supervised learning, the accuracy of prediction models (such as regression and classification models) was assessed against known benchmarks in the field of food waste forecasting. While the results were promising, the performance of the models could be improved through additional data and feature refinement, which is consistent with challenges highlighted in existing literature."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "DSAN-5000: Project",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\nIn conclusion, this project has provided valuable insights into the complex issue of food waste by employing a combination of unsupervised and supervised learning techniques. The results highlight both the potential and the limitations of using data science to address food waste. Key outcomes include the identification of clusters in the food waste data, the development of models for predicting economic impact, and the exploration of nutrient loss patterns. Despite these successes, several challenges remain, particularly in improving the precision of clustering results and refining the predictive models for greater accuracy.\nFuture work should focus on optimizing the models and exploring additional sources of data, such as socio-economic factors, geographic information, and consumer behavior, to enhance prediction accuracy. Further investigation into advanced machine learning techniques, such as ensemble models or neural networks, may also help improve model robustness. Additionally, expanding the dataset to include more diverse food categories and waste disposal methods would help create a more comprehensive understanding of the issue. Based on the results, it is clear that data-driven solutions hold great potential for informing policies and practices aimed at reducing food waste and its associated economic, environmental, and nutritional costs."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "This analysis employs a combination of exploratory data analysis (EDA), normalization, and statistical modeling techniques to draw insights from the data.\n\n\n• Data cleaning includes handling missing values, standardizing nutrient columns to consistent units (e.g., grams or milligrams per serving), and addressing outliers.\n• Visualization of key metrics like food surplus, waste, and nutrient content over time is used to identify patterns.\n\n\n\nNutritional data for protein, fat, carbs, fiber, vitamins, and minerals were often expressed in inconsistent units across the dataset (e.g., grams for macronutrients and milligrams for micronutrients). Standardizing these units enables meaningful comparisons across categories. For example:\n• Macronutrients (protein, fat, carbs) are standardized to grams per serving.\n• Micronutrients (iron, calcium, potassium) are converted to milligrams per serving.\n\n\n\n• Correlation analysis is used to identify links between food waste and nutrient density.\n• Regression models explore relationships between waste generation and external factors like supply and surplus.\n\n\n\n• Techniques like K-Means are applied to group food categories based on nutritional similarity, highlighting waste trends in nutrient-dense versus nutrient-poor foods.\nThese methods provide a robust framework for understanding food waste and its nutritional implications while offering actionable insights.\n\n\n\n• Everitt, B., Landau, S., Leese, M., & Stahl, D. (2011). Cluster Analysis. John Wiley & Sons.\n• Montgomery, D. C., Peck, E. A., & Vining, G. G. (2012). Introduction to Linear Regression Analysis. Wiley."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Introduction and Motivation",
    "section": "",
    "text": "Introduction and Motivation\nIn this analysis, I aim to explore the relationship between food waste, nutritional composition, and associated economic and environmental impacts. Specifically, I are building predictive models to address two key questions: (1) What factors contribute to food being classified as unfit for human consumption? and (2) How can I estimate the surplus value of food waste in monetary terms? By answering these questions, I aim to identify patterns and characteristics that can inform strategies for waste reduction and better resource allocation.\nThe analysis seeks to combine multiple domains, including nutrition, sustainability, and machine learning, to provide actionable insights. This work is motivated by the growing need to address food waste as a critical global challenge, with implications for food security, environmental sustainability, and economic efficiency.\n\n\nOverview of Methods\nThe exploratory data analysis (EDA) leverages a comprehensive set of techniques designed to uncover patterns, relationships, and insights within the dataset, supporting the broader goals of understanding food waste dynamics, nutritional composition, and economic impacts. These methods provide a foundation for addressing the project’s overarching objectives:\n\nDescriptive Analysis:\n• Techniques like univariate and bivariate visualizations (histograms, scatterplots, box plots) are employed to summarize key characteristics of the dataset, including waste metrics, water usage, and nutrient distributions. These insights lay the groundwork for identifying trends and outliers that may warrant further investigation.\nCorrelation and Relationship Mapping:\n• Correlation heatmaps and pairwise scatterplots reveal interdependencies among variables, such as the relationship between food waste quantities and nutritional properties. These insights help contextualize how different factors influence one another.\nClustering and Segmentation:\n• Clustering algorithms, like K-Means, group food categories based on their waste characteristics and nutritional profiles, providing a high-level understanding of similar food types and their shared challenges. This segmentation informs targeted interventions.\nDimensionality Reduction:\n• Methods such as Principal Component Analysis (PCA) reduce the complexity of the dataset, highlighting the most influential variables. This facilitates a clearer understanding of the main drivers behind food waste and economic losses.\nFeature Engineering and Insights Discovery:\n• Through calculated metrics and encoded variables (e.g., categorical encodings of food_category), new perspectives emerge, enhancing the interpretability of the data.\n\nEach method plays a role in linking data-driven insights to the overarching goals of reducing food waste, promoting nutritional balance, and minimizing economic losses. Together, these techniques provide a holistic view of the dataset, guiding both exploratory research and subsequent predictive modeling efforts."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Introduction and Motivation",
    "section": "Suggested page structure",
    "text": "Suggested page structure\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Introduction and Motivation",
    "section": "What to address",
    "text": "What to address\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "Use this page to track your progress and keep a log of your contributions to the project, please update this each time you work on your project, it is generally a good habit to adopt.\nIf you are working as a team, at the end, you can duplicate the project and add it to your individual portfolio websites. If you do, you MUST retain attribution to your teammates. Removing attribution would constitute plagiarism."
  },
  {
    "objectID": "technical-details/progress-log.html#to-do",
    "href": "technical-details/progress-log.html#to-do",
    "title": "Progress log",
    "section": "To-do",
    "text": "To-do\n\nExplore possible topics by brainstorming with GPT\nwrite a technical methods sections for K-means\nwrite a technical methods sections for PCA\n\n… etc"
  },
  {
    "objectID": "technical-details/progress-log.html#member-1",
    "href": "technical-details/progress-log.html#member-1",
    "title": "Progress log",
    "section": "Member-1:",
    "text": "Member-1:\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\nWeekly project contribution log:\nT: 10-15-2024\n\nCoordinate with team member to set up weekly meeting time\n\nM: 10-14-2024\n\nDo a first draft of the project landing page"
  },
  {
    "objectID": "technical-details/progress-log.html#member-2",
    "href": "technical-details/progress-log.html#member-2",
    "title": "Progress log",
    "section": "Member-2",
    "text": "Member-2\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\nWeekly project contribution log:\nW: 10-16-2024\n\nAttend first group meeting"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "For this project, data was collected primarily through APIs, which provided access to two key datasets: food-related nutritional data and food waste data. These datasets were fetched using requests to interact with the respective APIs, ensuring the collection of real-time and up-to-date information.\nThe food data was obtained from the FoodData Central API, which offered comprehensive details about food items, including nutritional attributes (e.g., protein, fats, carbohydrates, vitamins, minerals) and serving sizes. The food waste data was sourced from the ReFED API, which provided insights into food surplus and waste across various food categories, along with their environmental and economic impact.\nThe data collected from these APIs was then processed using Python libraries like pandas and cleaned to remove irrelevant columns, handle missing values, and standardize formats. This method of data collection ensured that the project had access to accurate, up-to-date, and comprehensive datasets to drive analysis on food waste patterns, nutritional content, and related metrics."
  },
  {
    "objectID": "technical-details/data-collection/main.html#suggested-page-structure",
    "href": "technical-details/data-collection/main.html#suggested-page-structure",
    "title": "Data Collection",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/main.html#what-to-address",
    "href": "technical-details/data-collection/main.html#what-to-address",
    "title": "Data Collection",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/main.html#start-collecting-data",
    "href": "technical-details/data-collection/main.html#start-collecting-data",
    "title": "Data Collection",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/main.html#saving-the-raw-data",
    "href": "technical-details/data-collection/main.html#saving-the-raw-data",
    "title": "Data Collection",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/main.html#requirements",
    "href": "technical-details/data-collection/main.html#requirements",
    "title": "Data Collection",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/main.html#example",
    "href": "technical-details/data-collection/main.html#example",
    "title": "Data Collection",
    "section": "Example",
    "text": "Example\nIn the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Step 1: Send a request to Wikipedia page\nurl = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\nresponse = requests.get(url)\n\n# Step 2: Parse the page content using BeautifulSoup\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Step 3: Find the table containing the data (usually the first table for such lists)\ntable = soup.find('table', {'class': 'wikitable'})\n\n# Step 4: Extract data from the table rows\ncountries = []\npopulations = []\n\n# Iterate over the table rows\nfor row in table.find_all('tr')[1:]:  # Skip the header row\n    cells = row.find_all('td')\n    if len(cells) &gt; 1:\n        country = cells[1].text.strip()  # The country name is in the second column\n        population = cells[2].text.strip()  # The population is in the third column\n        countries.append(country)\n        populations.append(population)\n\n# Step 5: Create a DataFrame to store the results\ndata = pd.DataFrame({\n    'Country': countries,\n    'Population': populations\n})\n\n# Display the scraped data\nprint(data)\n\n# Optionally save to CSV\ndata.to_csv('../../data/raw-data/countries_population.csv', index=False)\n\n                                 Country     Population\n0                                  World  8,119,000,000\n1                                  China  1,409,670,000\n2                          1,404,910,000          17.3%\n3                          United States    335,893,238\n4                              Indonesia    281,603,800\n..                                   ...            ...\n235                   Niue (New Zealand)          1,681\n236                Tokelau (New Zealand)          1,647\n237                         Vatican City            764\n238  Cocos (Keeling) Islands (Australia)            593\n239                Pitcairn Islands (UK)             35\n\n[240 rows x 2 columns]"
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\nDuring the course of this project, several technical challenges were encountered that impacted the analysis and interpretation of food waste data. One of the primary obstacles was the complexity and volume of the dataset, which included a wide range of food-related attributes, such as nutritional information, environmental impact, and waste characteristics. Handling missing data, inconsistent formatting, and discrepancies in unit measurements posed significant challenges during the preprocessing phase. Despite employing various data-cleaning techniques, some portions of the dataset still contained gaps or outliers that required careful treatment to avoid skewing the results.\nComputational limitations also became apparent when applying more complex machine learning algorithms, such as deep learning models or large-scale clustering techniques. These models required significant memory and processing power, particularly during the dimensionality reduction and clustering steps. Additionally, some algorithms, such as DBSCAN and hierarchical clustering, were sensitive to parameter tuning and often led to unexpected results, such as a high number of noise points or unclear cluster boundaries. This underlined the importance of selecting appropriate algorithms and fine-tuning their parameters for optimal results.\nUnexpected results, particularly in clustering and classification tasks, were observed when trying to identify distinct patterns in the food waste data. For instance, while PCA and t-SNE provided valuable insights into the structure of the data, they sometimes revealed overlapping clusters, making it difficult to draw definitive conclusions. These findings highlighted the inherent complexity of food waste patterns and suggested that more sophisticated feature engineering or additional data sources might be needed to improve model accuracy.\nFuture work should focus on refining the feature engineering process to account for the broader contextual factors surrounding food waste, such as geographical location, socioeconomic status, and consumer behavior. Additionally, improving data quality through better standardization and exploring alternative machine learning models, such as ensemble techniques or advanced deep learning models, could enhance performance. Scaling the solution to handle larger datasets with more granular information would also be an important next step."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\nThe results of this project were compared to relevant industry benchmarks and existing research on food waste management and prediction. In general, the findings aligned with expectations from similar research, which suggests that food waste follows complex patterns influenced by various factors, such as nutritional content, environmental footprint, and supply chain dynamics.\nIn terms of supervised learning, the accuracy of prediction models (such as regression and classification models) was assessed against known benchmarks in the field of food waste forecasting. While the results were promising, the performance of the models could be improved through additional data and feature refinement, which is consistent with challenges highlighted in existing literature."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\nIn conclusion, this project has provided valuable insights into the complex issue of food waste by employing a combination of unsupervised and supervised learning techniques. The results highlight both the potential and the limitations of using data science to address food waste. Key outcomes include the identification of clusters in the food waste data, the development of models for predicting economic impact, and the exploration of nutrient loss patterns. Despite these successes, several challenges remain, particularly in improving the precision of clustering results and refining the predictive models for greater accuracy.\nFuture work should focus on optimizing the models and exploring additional sources of data, such as socio-economic factors, geographic information, and consumer behavior, to enhance prediction accuracy. Further investigation into advanced machine learning techniques, such as ensemble models or neural networks, may also help improve model robustness. Additionally, expanding the dataset to include more diverse food categories and waste disposal methods would help create a more comprehensive understanding of the issue. Based on the results, it is clear that data-driven solutions hold great potential for informing policies and practices aimed at reducing food waste and its associated economic, environmental, and nutritional costs."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "While unsupervised learning helps uncover patterns in food waste data, supervised learning can provide actionable predictions that directly inform decision-making processes. Supervised learning techniques, such as classification and regression, are essential for predicting outcomes like the surplus dollar value of wasted food or classifying food donations based on nutritional composition. By training models on labeled data, we can predict which food items are most likely to be wasted, which can help inform better resource allocation and waste reduction strategies.\nIn this project, supervised learning will be used to predict the economic impact of food waste by estimating the dollar surplus associated with surplus food. This involves building regression models that take into account factors such as the nutritional content, environmental footprint, and food category to forecast economic losses. Additionally, classification models will be employed to identify which food donations are most suitable for redistribution based on their nutritional value and waste characteristics. These insights will guide the development of policies and practices to reduce food waste and its associated environmental and economic costs. By combining supervised learning with unsupervised insights, this project aims to create a comprehensive framework for mitigating food waste through data-driven strategies."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "title": "Supervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-to-address",
    "href": "technical-details/supervised-learning/main.html#what-to-address",
    "title": "Supervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#data-preprocessing",
    "href": "technical-details/supervised-learning/main.html#data-preprocessing",
    "title": "Supervised Learning",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-selection",
    "href": "technical-details/supervised-learning/main.html#model-selection",
    "title": "Supervised Learning",
    "section": "",
    "text": "The choice of models—K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machines (SVM)—was guided by their complementary strengths.\n• KNN: Selected for its simplicity and effectiveness in handling multiclass problems with non-linear decision boundaries.\n• Random Forest: Chosen for its robust performance across diverse datasets, ability to handle high-dimensional data, and built-in feature importance evaluation.\n• SVM: Included for its strength in finding optimal hyperplanes in high-dimensional spaces and its kernel flexibility for complex decision boundaries.\n\n\n\n• KNN: Classifies samples by majority voting among k nearest neighbors in the feature space, using distance metrics like Manhattan or Euclidean.\n• Random Forest: Constructs multiple decision trees during training, averaging their outputs for classification to reduce overfitting and variance.\n• SVM: Uses a hyperplane to separate classes with maximum margin and can apply non-linear kernels such as RBF for complex classification tasks.\nTraining and Testing Strategy\n\n\n\nThe dataset was divided into training and testing subsets using a train-test split, ensuring that the model’s performance generalizes to unseen data. Additionally, cross-validation (GridSearchCV) was used during hyperparameter tuning to assess the model’s stability and avoid overfitting.\n\n\n\nThe dataset was split into 80% training data and 20% testing data to ensure adequate data for both model training and evaluation.\n\n\n\nNormalization\nA StandardScaler was applied to normalize the feature variables, ensuring that all features have zero mean and unit variance. This step was particularly critical for algorithms like SVM and KNN, which are sensitive to the scale of input data.\nBalancing the Dataset\nClass imbalances were addressed using SMOTE (Synthetic Minority Oversampling Technique) combined with ENN (Edited Nearest Neighbors) via SMOTEENN. This method oversamples the minority classes and removes noisy samples near class boundaries, ensuring balanced class representation in the training data."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "title": "Supervised Learning",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "title": "Supervised Learning",
    "section": "",
    "text": "• Accuracy: Measures the percentage of correctly classified samples across all classes.\n• Precision, Recall, F1-Score: Evaluates the model’s performance for each class, balancing false positives and false negatives.\n• Confusion Matrix: Used to visualize the distribution of predictions across classes.\n• Cross-Validation Score: Captures model consistency by evaluating performance across multiple data splits."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "",
    "text": "KNN:\n\n• Best Cross-Validation Score: 0.986\n• Classification Report: Demonstrated strong performance, particularly for large classes like landfill, while showing some variability for minority classes such as animal_feed.\n\nRandom Forest:\n\n• Best Cross-Validation Score: 0.993\n• Classification Report: Delivered excellent performance with well-balanced precision and recall across all classes, reflecting its robust nature in handling imbalanced datasets.\n\nSVM:\n\n• Best Cross-Validation Score: 0.944\n• Classification Report: Achieved reasonable performance but struggled with smaller classes like animal_feed, likely due to limited data and the complexity of separating the classes."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#discussion",
    "href": "technical-details/supervised-learning/main.html#discussion",
    "title": "Supervised Learning",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses",
    "section": "",
    "text": "Food waste represents a critical issue at the intersection of sustainability, public health, and economics. Globally, one-third of all food produced is wasted, leading to significant environmental strain, economic loss, and missed opportunities to alleviate hunger. Addressing this challenge requires an interdisciplinary approach, leveraging data science to analyze and inform actionable solutions. My project, Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses, aims to bridge these dimensions and provide a comprehensive framework for understanding and mitigating food waste.\nThrough this project, I aim to classify food donations based on nutrient composition, predict dollar surplus associated with surplus food, and explore primary disposal methods using advanced statistical and machine learning techniques. These efforts will contribute to smarter policies and practices for a more sustainable future."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses",
    "section": "About Me",
    "text": "About Me\n\n\n\nShanay Wadhwani\n\n\nHi, I’m Shanay Wadhwani, a data scientist and graduate student at Georgetown University, currently pursuing a Master of Science in Data Science and Analytics. My academic and professional journey is guided by a strong commitment to using data for meaningful societal change, particularly in sustainability and public health. This passion drives my current project, “Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses.”\nThis project focuses on exploring the multifaceted impact of food waste, delving into its economic costs, nutritional losses, and environmental consequences. By leveraging advanced data science techniques like Principal Component Analysis (PCA), clustering algorithms, and predictive modeling, I aim to classify food categories based on waste patterns and identify actionable insights to mitigate waste. My goal is to develop a comprehensive statistical interface that informs stakeholders about the ripple effects of food waste and highlights pathways for sustainability. Through this work, I hope to promote smarter policies and practices in food systems, from farm to table.\nAt Georgetown, I’ve cultivated a strong foundation in Natural Language Processing (NLP), probabilistic modeling, and computational linguistics, complemented by my leadership roles as a Graduate Student Senator and Student Ambassador. My role as a Teaching Assistant for the Data Mining course has deepened my ability to collaborate and mentor others, qualities I bring to every project I undertake.\nProfessionally, I’ve applied my expertise to diverse domains, including ETL development, AI-driven solutions, and financial modeling. At Argus Media, I spearheaded the development of ETL pipelines using the Golem framework, streamlining the processing of over a million pricing entries monthly while managing a team of 11 engineers. This experience honed my skills in project management and workflow optimization.\nMy technical toolkit includes Python, R, SQL, TensorFlow, Keras, ETL, and AWS, enabling me to solve complex problems in machine learning, deep learning, and statistical modeling. Whether it’s enhancing financial forecasting models at ClearPrism or exploring sustainability in my current research, I strive to combine technical precision with real-world impact.\nThe “Nutrient Waste” project reflects my broader mission to leverage data science for sustainable change. By analyzing the intersections of food waste, nutrition, and environmental resources, I aim to contribute to a more sustainable and equitable future. If you’re interested in this work or exploring potential collaborations, I’d love to connect. You can reach me at sw1449@georgetown.edu or explore my projects on my GitHub Profile. Let’s create solutions that matter!"
  },
  {
    "objectID": "index.html#literature-review-leveraging-data-science-for-food-waste-analysis",
    "href": "index.html#literature-review-leveraging-data-science-for-food-waste-analysis",
    "title": "Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses",
    "section": "Literature Review: Leveraging Data Science for Food Waste Analysis",
    "text": "Literature Review: Leveraging Data Science for Food Waste Analysis\nThe problem of food waste is a pressing global challenge, with significant environmental, economic, and social implications. Research in this domain increasingly focuses on utilizing data science techniques to uncover patterns, predict outcomes, and inform decision-making processes. This literature review synthesizes key studies relevant to our objectives: classifying food donations based on nutrient content, building a regression model for dollar surplus, and predicting primary disposal methods.\n\n\n\nHere, recycling refers to food waste which is composted, used as land applications, used for industrial purposes, for anaerobic digestion, or for animal feed.\nFood Waste on the other hand refers to food waste which is dumped in a landfill, a sewer, is not harvested or is incinerated\nFood Waste and Nutrient Analysis\nFood waste analysis often incorporates nutritional dimensions to understand its broader implications. Researchers like Beretta et al. (2013) highlight the nutritional losses associated with food waste, emphasizing the need to connect food surplus with nutrient availability for donation purposes. Studies leveraging machine learning, such as those by Fiore et al. (2020), demonstrate the utility of classification algorithms in categorizing waste by food type and nutrient content. These approaches can guide interventions to redistribute surplus food to populations in need while minimizing waste.\nEconomic Valuation of Food Surplus\nEconomic studies, such as those by Gunders et al. (2017), have assessed the dollar value of food surplus and waste, shedding light on the financial implications of inefficiencies in the food system. Regression models are frequently applied to estimate surplus value, considering variables like production costs, market value, and transportation. These models provide actionable insights for optimizing redistribution strategies and reducing financial losses.\nDisposal Method Classification\nIdentifying the primary disposal method for food waste is critical for environmental sustainability. Studies like those by Papargyropoulou et al. (2014) have categorized disposal methods—such as landfill, composting, and anaerobic digestion—based on factors like waste type, volume, and location. Machine learning classification models, as demonstrated by Song et al. (2018), are particularly effective in predicting disposal methods, combining categorical features with waste composition data.\nData Science Applications in Food Systems\nRecent advancements in data science tools and techniques have significantly impacted food waste research. Unsupervised learning methods, including Principal Component Analysis (PCA) and clustering algorithms, have been employed to uncover patterns in food waste characteristics (e.g., nutrient composition and disposal methods). Supervised learning, including regression and classification, has been applied to forecast outcomes like economic surplus and redistribution potential.\nGaps and Opportunities\nWhile substantial progress has been made, there are gaps in integrating nutritional, economic, and environmental dimensions into a unified analytical framework. Few studies explicitly combine nutrient profiling with economic surplus modeling or disposal classification. Bridging these dimensions could yield a comprehensive approach to managing food waste more effectively. Additionally, the inclusion of socio-economic factors, such as accessibility and donation feasibility, remains underexplored.\nRelevance to Current Research\nThis body of work informs our project’s design in the following ways: 1. Classification of Food Donations: Drawing on techniques from Fiore et al. (2020), we aim to build nutrient-based classification models to optimize food redistribution. 2. Regression Modeling for Dollar Surplus: Guided by Gunders et al. (2017), we will leverage regression analysis to quantify the economic value of surplus food. 3. Disposal Method Prediction: Inspired by Song et al. (2018), we intend to develop a predictive model for disposal methods, incorporating both categorical and continuous data.\nMy project seeks to advance the integration of these dimensions, addressing gaps identified in the literature and contributing to a holistic understanding of food waste management."
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses",
    "section": "Methods",
    "text": "Methods\nIn this project, I am using advanced data science techniques to analyze food waste from multiple perspectives:\n\nClassification Models: I am applying supervised learning techniques to classify food donations based on their nutrient profiles.\nRegression Analysis: I am building regression models to predict the dollar surplus of food waste, considering physical waste characteristics, water footprints, and nutritional attributes.\nDisposal Method Prediction: I am using classification models to identify the primary disposal method for food waste, allowing me to develop targeted interventions to reduce landfill contributions.\nDimensionality Reduction and Clustering: I am utilizing techniques like PCA and clustering to uncover latent patterns in food waste data and segment food categories by waste characteristics.\nData Integration and Cleaning: I am combining diverse datasets, addressing missing values, and ensuring a solid foundation for analysis."
  },
  {
    "objectID": "index.html#call-to-action",
    "href": "index.html#call-to-action",
    "title": "Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses",
    "section": "Call to Action",
    "text": "Call to Action\nI’m working on tackling one of the most pressing global challenges: food waste. Through a combination of innovative data science methodologies and a focus on sustainability, this project aims to provide actionable insights and support informed decision-making.\nIf you’re passionate about sustainability, data science, or food systems, I would love to collaborate. Whether you’re a researcher, policymaker, or organization aiming to reduce food waste, let’s work together to create impactful solutions. Feel free to reach out and connect with me on here sw1449@georgetown.edu!"
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Nutrient Waste: Analyzing the Journey of Food Waste Through Economic, Environmental, and Nutritional Lenses",
    "section": "Research Questions",
    "text": "Research Questions\nThis project attempts to answer the following research questions:\n\nHow can we classify food donations based on the nutrient profiles of surplus food items?\n• This question focuses on building machine learning models to categorize surplus food by its nutritional content, enabling more effective redistribution strategies.\nWhat is the economic surplus value (in dollars) associated with specific food categories, and how can we predict it?\n• A regression model will estimate the dollar surplus of food waste, offering insights into economic losses and potential cost-saving strategies.\nWhat are the primary disposal methods for various types of food waste, and what factors determine these methods?\n• Classification models will help identify and predict disposal methods based on categorical and continuous features of food waste.\nWhat patterns emerge when analyzing food waste through clustering and dimensionality reduction techniques like PCA and t-SNE?\n• These methods will uncover latent structures in the dataset, informing further exploratory and predictive modeling efforts.\nHow do waste patterns vary across food categories, and which attributes most significantly impact food’s fitness for human consumption?\n• By exploring multivariate relationships, this question examines the drivers of waste classification and surplus prediction."
  },
  {
    "objectID": "technical-details/eda/main.html#eda-summary",
    "href": "technical-details/eda/main.html#eda-summary",
    "title": "Exploratory Data Analysis",
    "section": "EDA Summary",
    "text": "EDA Summary\n\nCorrelation between Serving Size and Waste:\n\n• The plot of serving size against uneaten food shows a positive correlation, suggesting that larger serving sizes are associated with higher amounts of waste. The significant p-value confirms that this relationship is statistically robust. This implies that adjusting portion sizes could be an effective strategy to reduce food waste.\n\nDistribution of Calories:\n\n• The histogram reveals two distinct peaks in the distribution of calories, one in the 0-24 calorie range and another around 250 calories. The presence of outliers likely represents high-calorie meal-prep foods. This highlights the need for targeted interventions to reduce waste in high-calorie food items.\n\nMacronutrient Relationships:\n\n• The pair plot of macronutrients shows strong positive correlations between calories, fats, and carbs. Foods high in fats tend to also be high in carbs, while protein-rich foods generally have lower carb content. This insight can help guide interventions by identifying food categories that are resource-intensive and contribute to both environmental and economic waste.\n\nFood Disposal Methods:\n\n• The countplot of food waste disposal methods reveals that most food waste ends up in landfills, followed by composting, sewer waste, and animal feed recycling. This highlights the potential for nutrition-based interventions to reduce waste by shifting food from landfills to more sustainable disposal or repurposing methods.\n\nCorrelations with Economic Surplus and Environmental Impact:\n\n• The correlation heatmap indicates that high protein and high fat foods are associated with higher environmental costs, in terms of water footprint, and contribute to significant economic losses when wasted. This suggests that reducing waste in these food categories could lead to both environmental and economic benefits.\n• Additionally, food waste that is recycled tends to overlap with food that could be donated, showing that improving donation systems could further reduce waste and benefit communities.\n\nBoxplot Insights on Waste by Caloric Value:\n\n• Boxplots show that higher-calorie foods are predominantly wasted in landfills or as animal feed, while lower-calorie foods are more likely to be composted or not harvested. This suggests that interventions targeting high-calorie foods could help mitigate their environmental and economic impacts.\n\nWater Footprint and Protein-Rich Foods:\n\n• The scatter plot with a trendline shows a positive correlation between protein-rich foods and their water footprint. This indicates that foods like meats, eggs, and dairy, which have high protein content, are resource-intensive to produce and contribute significantly to environmental degradation when wasted.\n\nEconomic Surplus and Protein-Rich Foods:\n\n• The scatter plot of protein content versus US dollars surplus indicates that protein-rich foods also contribute significantly to economic loss when wasted. Reducing waste in these categories could alleviate substantial financial losses.\n\nFiber Content and Recycling:\n\n• The scatter plot shows a weak positive correlation between fiber content and tons recycled, suggesting that fiber-rich foods, often organic, are more likely to be recycled. This may indicate that organic food waste has a higher chance of being repurposed or recycled.\n\nFood Rescue and Donation:\n\n• The hex bin plot suggests that foods with caloric values between 50-120 and 250-400 calories are most commonly donated. This points to specific food types that are frequently rescued and redistributed, possibly due to their suitability for donation and lower perishability.\n\nCaloric Value and Recycling:\n\n• The KDE plot indicates that foods with caloric values between 80-160 calories are most likely to be recycled, emphasizing that low-calorie foods are more easily repurposed when wasted. This suggests that low-calorie foods may have a higher chance of being repurposed through recycling or donation programs.\n\nEconomic Surplus in Specific Serving Sizes:\n\n• The KDE of calories vs. tons donated shows that certain foods with serving sizes of 40-60 grams and 150-200 grams contribute to substantial economic surplus when wasted. This may point to specific types of foods that, despite their serving size, result in significant financial loss when wasted."
  },
  {
    "objectID": "technical-details/eda/main.html#implications",
    "href": "technical-details/eda/main.html#implications",
    "title": "Exploratory Data Analysis",
    "section": "Implications:",
    "text": "Implications:\nThe findings highlight several key areas where interventions could reduce food waste:\n• Portion Control: Reducing portion sizes could lower the amount of food wasted, especially for larger servings.\n• Targeted Nutrition-Based Interventions: Focusing on high-calorie and protein-rich foods could reduce environmental and economic waste.\n• Recycling and Donation Programs: Encouraging the donation of low-calorie foods and improving recycling systems for fiber-rich foods could help repurpose waste.\n• Consumer and Organizational Action: Insights into food disposal methods and the impact of serving sizes can guide consumer behavior and organizational policies aimed at reducing food waste at all stages of the supply chain."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#multi-class-classification",
    "href": "technical-details/supervised-learning/main.html#multi-class-classification",
    "title": "Supervised Learning",
    "section": "Multi Class Classification",
    "text": "Multi Class Classification\nThis implementation aims to predict the disposal method—the primary way a specific type of food waste is handled—using machine learning models in a multiclass classification framework. The target variable is disposal_method, which includes categories such as landfill, composting, animal_feed, not_harvested, and sewer.\nThe feature variables used for prediction are:\n• Physical Waste Characteristics: tons_surplus, tons_supply, tons_waste, tons_inedible_parts\n• Environmental Impact Factor: gallons_water_footprint\n• Nutritional Composition: serving_size, calories, protein, fat, carbs, fiber, calcium, iron, sodium\nBy leveraging these features, the project seeks to identify patterns in waste management, enabling better decision-making for sustainability and resource optimization across the food supply chain.\n\nfeature_columns = [\n    'tons_surplus', 'tons_supply', 'tons_waste', 'tons_inedible_parts',\n    'gallons_water_footprint', 'serving_size', 'calories', 'protein', 'fat', 'carbs',\n    'fiber', 'calcium', 'iron', 'sodium'\n]\ntarget_column = 'disposal_method'  \nlearner = SupervisedLearning(data)\nlearner.prepare_data(feature_columns, target_column)\n\nprint(\"\\n--- KNN Model Testing ---\")\nknn_result = learner.multiclass_classification(\n    input_features=feature_columns,\n    target_feature=target_column,\n    model_type=\"knn\"\n)\n\nprint(\"Trained KNN Model Parameters:\", knn_result[\"model\"].get_params())\n\nprint(\"\\n--- Random Forest Model Testing ---\")\n\nrandom_forest_result = learner.multiclass_classification(\n    input_features=feature_columns,\n    target_feature=target_column,\n    model_type=\"random_forest\"\n)\n\n# Best Parameters and Model Output\nprint(\"Trained Model Parameters:\", random_forest_result[\"model\"].get_params())\n# SVM Testing\nprint(\"\\n--- SVM Model Testing ---\")\nsvm_result = learner.multiclass_classification(\n    input_features=feature_columns,\n    target_feature=target_column,\n    model_type=\"svm\"\n)\n\nprint(\"Trained SVM Model Parameters:\", svm_result[\"model\"].get_params())\n\n\n--- KNN Model Testing ---\nBest Parameters: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}\nBest Cross-Validation Score: 0.9859447031830759\nClassification Report:\n                precision    recall  f1-score   support\n\n  animal_feed       1.00      1.00      1.00         3\n   composting       0.79      0.95      0.86       123\n     landfill       0.99      0.92      0.95       961\nnot_harvested       0.80      0.97      0.88       137\n        sewer       0.75      1.00      0.86        39\n\n     accuracy                           0.93      1263\n    macro avg       0.87      0.97      0.91      1263\n weighted avg       0.94      0.93      0.93      1263\n\n\n\n\n\n\n\n\n\n\nTrained KNN Model Parameters: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'manhattan', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n\n--- Random Forest Model Testing ---\nBest Parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\nBest Cross-Validation Score: 0.9934739579764355\nClassification Report:\n                precision    recall  f1-score   support\n\n  animal_feed       1.00      1.00      1.00         3\n   composting       0.83      0.94      0.88       123\n     landfill       0.99      0.92      0.95       961\nnot_harvested       0.72      0.96      0.82       137\n        sewer       0.87      1.00      0.93        39\n\n     accuracy                           0.93      1263\n    macro avg       0.88      0.97      0.92      1263\n weighted avg       0.94      0.93      0.93      1263\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrained Model Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n\n--- SVM Model Testing ---\nBest Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\nBest Cross-Validation Score: 0.9444031722852726\nClassification Report:\n                precision    recall  f1-score   support\n\n  animal_feed       0.50      1.00      0.67         3\n   composting       0.41      0.97      0.58       123\n     landfill       0.99      0.76      0.86       961\nnot_harvested       0.55      0.72      0.62       137\n        sewer       0.72      1.00      0.84        39\n\n     accuracy                           0.78      1263\n    macro avg       0.64      0.89      0.71      1263\n weighted avg       0.88      0.78      0.81      1263\n\n\n\n\n\n\n\n\n\n\nTrained SVM Model Parameters: {'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 42, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n\n\n\nModel Evaluation Metrics\n\nMulticlass Classification Metrics\n• Accuracy: Measures the percentage of correctly classified samples across all classes.\n• Precision, Recall, F1-Score: Evaluates the model’s performance for each class, balancing false positives and false negatives.\n• Confusion Matrix: Used to visualize the distribution of predictions across classes.\n• Cross-Validation Score: Captures model consistency by evaluating performance across multiple data splits.\n\n\n\nResults\n\nKNN:\n• Best Cross-Validation Score: 0.986\n• Classification Report: Demonstrated strong performance, particularly for large classes like landfill, while showing some variability for minority classes such as animal_feed.\nRandom Forest:\n• Best Cross-Validation Score: 0.993\n• Classification Report: Delivered excellent performance with well-balanced precision and recall across all classes, reflecting its robust nature in handling imbalanced datasets.\nSVM:\n• Best Cross-Validation Score: 0.944\n• Classification Report: Achieved reasonable performance but struggled with smaller classes like animal_feed, likely due to limited data and the complexity of separating the classes.\n\n\n\nImplications\nThe results of this project provide valuable insights into food waste management practices. Accurate classification of disposal methods based on physical, environmental, and nutritional factors can:\n• Enable targeted policy interventions for waste reduction.\n• Inform stakeholders about the environmental impact of different disposal methods.\n• Support resource allocation for improving sustainable waste management practices.\nFuture efforts could include expanding the dataset, incorporating additional features (e.g., geographic data), and exploring advanced ensemble techniques for even greater predictive accuracy."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#implications",
    "href": "technical-details/supervised-learning/main.html#implications",
    "title": "Supervised Learning",
    "section": "",
    "text": "The results of this project provide valuable insights into food waste management practices. Accurate classification of disposal methods based on physical, environmental, and nutritional factors can:\n• Enable targeted policy interventions for waste reduction.\n• Inform stakeholders about the environmental impact of different disposal methods.\n• Support resource allocation for improving sustainable waste management practices.\nFuture efforts could include expanding the dataset, incorporating additional features (e.g., geographic data), and exploring advanced ensemble techniques for even greater predictive accuracy.\n\ninput_features = [\n    'gallons_water_footprint',\n    'serving_size',\n    'protein',\n    'fat',\n    'carbs',\n    'calories',\n    'fiber',\n    'calcium',\n    'iron',\n    'sodium',\n]\n\n# Target feature (donation_bin)\ntarget_feature = 'donation_bin'\n\nsl = SupervisedLearning(data)\n\n# Prepare the data for training (this will split the data and apply SMOTE if required)\nsl.prepare_data(input_features, target_feature)\nprint(\"\\n--- KNN Model Testing ---\")\n\n# Perform binary classification with random forest (you can choose other models as well)\nresults_knn = sl.binary_classification(input_features, target_feature, model_type=\"knn\", use_scaling=True)\n\n# Output the results\nprint(results_knn)\n\nprint(\"\\n--- Random Forest Model Testing ---\")\n\nresults_random_forest = sl.binary_classification(input_features, target_feature, model_type=\"random_forest\", use_scaling=True)\n\n# Output the results\nprint(results_random_forest)\n\nprint(\"\\n--- SVM Model Testing ---\")\n\nresults_svm = sl.binary_classification(input_features, target_feature, model_type=\"svm\", use_scaling=True)\n\n# Output the results\nprint(results_svm)\n\n\n\n--- KNN Model Testing ---\nBest Parameters: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\nBest Cross-Validation Score: 0.9394957867108717\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.88      0.85      0.87       609\n           1       0.87      0.90      0.88       688\n\n    accuracy                           0.88      1297\n   macro avg       0.88      0.87      0.88      1297\nweighted avg       0.88      0.88      0.88      1297\n\n\n\n\n\n\n\n\n\n\n{'model': KNeighborsClassifier(metric='euclidean', n_neighbors=3, weights='distance'), 'classification_report': {'0': {'precision': 0.8822525597269625, 'recall': 0.8489326765188834, 'f1-score': 0.8652719665271966, 'support': 609.0}, '1': {'precision': 0.870604781997187, 'recall': 0.8997093023255814, 'f1-score': 0.8849177984274482, 'support': 688.0}, 'accuracy': 0.8758673862760216, 'macro avg': {'precision': 0.8764286708620748, 'recall': 0.8743209894222324, 'f1-score': 0.8750948824773224, 'support': 1297.0}, 'weighted avg': {'precision': 0.876073939003689, 'recall': 0.8758673862760216, 'f1-score': 0.8756931942429816, 'support': 1297.0}}, 'confusion_matrix': array([[517,  92],\n       [ 69, 619]])}\n\n--- Random Forest Model Testing ---\n\n\n/Users/shanaywadhwani/Desktop/anaconda3/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n  _data = np.array(data, dtype=dtype, copy=copy,\n\n\nBest Parameters: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\nBest Cross-Validation Score: 0.965279734769996\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.86      0.88      0.87       609\n           1       0.89      0.87      0.88       688\n\n    accuracy                           0.87      1297\n   macro avg       0.87      0.87      0.87      1297\nweighted avg       0.87      0.87      0.87      1297\n\n\n\n\n\n\n\n\n\n\n{'model': RandomForestClassifier(class_weight='balanced', max_depth=15, n_estimators=300,\n                       random_state=42), 'classification_report': {'0': {'precision': 0.8585209003215434, 'recall': 0.8768472906403941, 'f1-score': 0.867587327376117, 'support': 609.0}, '1': {'precision': 0.8888888888888888, 'recall': 0.872093023255814, 'f1-score': 0.880410858400587, 'support': 688.0}, 'accuracy': 0.874325366229761, 'macro avg': {'precision': 0.8737048946052162, 'recall': 0.874470156948104, 'f1-score': 0.873999092888352, 'support': 1297.0}, 'weighted avg': {'precision': 0.8746297485361414, 'recall': 0.874325366229761, 'f1-score': 0.8743896321909476, 'support': 1297.0}}, 'confusion_matrix': array([[534,  75],\n       [ 88, 600]])}\n\n--- SVM Model Testing ---\nBest Parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\nBest Cross-Validation Score: 0.7290716949854953\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.76      0.43      0.55       609\n           1       0.64      0.88      0.74       688\n\n    accuracy                           0.67      1297\n   macro avg       0.70      0.66      0.64      1297\nweighted avg       0.69      0.67      0.65      1297\n\n\n\n\n\n\n\n\n\n\n{'model': SVC(C=10, class_weight='balanced', gamma='auto', random_state=42), 'classification_report': {'0': {'precision': 0.7586206896551724, 'recall': 0.43349753694581283, 'f1-score': 0.5517241379310345, 'support': 609.0}, '1': {'precision': 0.6364594309799789, 'recall': 0.877906976744186, 'f1-score': 0.7379352474037875, 'support': 688.0}, 'accuracy': 0.669236700077101, 'macro avg': {'precision': 0.6975400603175756, 'recall': 0.6557022568449995, 'f1-score': 0.644829692667411, 'support': 1297.0}, 'weighted avg': {'precision': 0.6938196518999425, 'recall': 0.669236700077101, 'f1-score': 0.6505007326243684, 'support': 1297.0}}, 'confusion_matrix': array([[264, 345],\n       [ 84, 604]])}\n\n\n\n# Instantiate the class\nsl = SupervisedLearning(data)\n\n# One-hot encode the 'food_category' column\n#sl.one_hot_encode('food_category')\n\n# Define input and target features\ninput_features = [\n    'gallons_water_footprint', 'protein', 'fat', 'carbs', 'calories', 'fiber',\n    'calcium', 'iron', 'sodium'\n] #+ [col for col in sl.data.columns if col.startswith('food_category_')]  \ntarget_feature = 'us_dollars_surplus'\n\n# Prepare the data\nsl.prepare_data(input_features, target_feature, cat_target = False)\n\n# Perform regression\nprint(\"\\n--- Random Forest Model Testing ---\")\n\nresults_rf = sl.regression(input_features, target_feature, model_type=\"random_forest\", use_scaling=True)\nprint(results_rf)\n\n# Perform regression\nprint(\"\\n--- Linear Reggression Model Testing ---\")\n\nresults_lr = sl.regression(input_features, target_feature, model_type=\"linear_regression\", use_scaling=True)\nprint(results_lr)\n\nSkipping SMOTEENN as one or more classes have fewer than 2 samples.\n\n--- Random Forest Model Testing ---\n\n\n/Users/shanaywadhwani/Desktop/anaconda3/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n  _data = np.array(data, dtype=dtype, copy=copy,\n\n\nBest Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\nBest Cross-Validation Score: 0.9709478039104822\nMean Squared Error (MSE): 8623858527331103.0\nMean Absolute Error (MAE): 31635667.66509777\nR-squared (R2): 0.9746811131128484\n{'model': RandomForestRegressor(max_depth=20, max_features='sqrt', n_estimators=300,\n                      random_state=42), 'mse': 8623858527331103.0, 'mae': 31635667.66509777, 'r2': 0.9746811131128484}\n\n--- Linear Reggression Model Testing ---\nMean Squared Error (MSE): 1.6665692781340918e+17\nMean Absolute Error (MAE): 221610068.59841296\nR-squared (R2): 0.5107099808171649\n{'model': LinearRegression(), 'mse': 1.6665692781340918e+17, 'mae': 221610068.59841296, 'r2': 0.5107099808171649}"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#binary-classification",
    "href": "technical-details/supervised-learning/main.html#binary-classification",
    "title": "Supervised Learning",
    "section": "Binary Classification",
    "text": "Binary Classification\nHere, we aim to predict whether food will be donated or not based on its nutritional and physical attributes. The task is framed as a binary classification problem, where the target variable is donation_bin, which indicates if a food item is suitable for donation (1) or not (0).\nThe input features used for prediction include various food-related characteristics, such as:\n• Environmental Impact Factor: gallons_water_footprint\n• Nutritional Composition: serving_size, calories, protein, fat, carbs, fiber, calcium, iron, sodium\nThese features are intended to capture the physical and nutritional profile of food that may influence whether it’s fit for donation. The target variable is donation_bin, which represents whether the food item is suitable for donation\n\ninput_features = [\n    'gallons_water_footprint',\n    'serving_size',\n    'protein',\n    'fat',\n    'carbs',\n    'calories',\n    'fiber',\n    'calcium',\n    'iron',\n    'sodium',\n]\n\n# Target feature (donation_bin)\ntarget_feature = 'donation_bin'\n\nsl = SupervisedLearning(data)\n\n# Prepare the data for training (this will split the data and apply SMOTE if required)\nsl.prepare_data(input_features, target_feature)\nprint(\"\\n--- KNN Model Testing ---\")\n\n# Perform binary classification with random forest (you can choose other models as well)\nresults_knn = sl.binary_classification(input_features, target_feature, model_type=\"knn\", use_scaling=True)\n\n# Output the results\nprint(results_knn)\n\nprint(\"\\n--- Random Forest Model Testing ---\")\n\nresults_random_forest = sl.binary_classification(input_features, target_feature, model_type=\"random_forest\", use_scaling=True)\n\n# Output the results\nprint(results_random_forest)\n\nprint(\"\\n--- SVM Model Testing ---\")\n\nresults_svm = sl.binary_classification(input_features, target_feature, model_type=\"svm\", use_scaling=True)\n\n# Output the results\nprint(results_svm)\n\n\n\n--- KNN Model Testing ---\nBest Parameters: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\nBest Cross-Validation Score: 0.9394957867108717\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.88      0.85      0.87       609\n           1       0.87      0.90      0.88       688\n\n    accuracy                           0.88      1297\n   macro avg       0.88      0.87      0.88      1297\nweighted avg       0.88      0.88      0.88      1297\n\n\n\n\n\n\n\n\n\n\n{'model': KNeighborsClassifier(metric='euclidean', n_neighbors=3, weights='distance'), 'classification_report': {'0': {'precision': 0.8822525597269625, 'recall': 0.8489326765188834, 'f1-score': 0.8652719665271966, 'support': 609.0}, '1': {'precision': 0.870604781997187, 'recall': 0.8997093023255814, 'f1-score': 0.8849177984274482, 'support': 688.0}, 'accuracy': 0.8758673862760216, 'macro avg': {'precision': 0.8764286708620748, 'recall': 0.8743209894222324, 'f1-score': 0.8750948824773224, 'support': 1297.0}, 'weighted avg': {'precision': 0.876073939003689, 'recall': 0.8758673862760216, 'f1-score': 0.8756931942429816, 'support': 1297.0}}, 'confusion_matrix': array([[517,  92],\n       [ 69, 619]])}\n\n--- Random Forest Model Testing ---\nBest Parameters: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\nBest Cross-Validation Score: 0.965279734769996\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.86      0.88      0.87       609\n           1       0.89      0.87      0.88       688\n\n    accuracy                           0.87      1297\n   macro avg       0.87      0.87      0.87      1297\nweighted avg       0.87      0.87      0.87      1297\n\n\n\n\n\n\n\n\n\n\n{'model': RandomForestClassifier(class_weight='balanced', max_depth=15, n_estimators=300,\n                       random_state=42), 'classification_report': {'0': {'precision': 0.8585209003215434, 'recall': 0.8768472906403941, 'f1-score': 0.867587327376117, 'support': 609.0}, '1': {'precision': 0.8888888888888888, 'recall': 0.872093023255814, 'f1-score': 0.880410858400587, 'support': 688.0}, 'accuracy': 0.874325366229761, 'macro avg': {'precision': 0.8737048946052162, 'recall': 0.874470156948104, 'f1-score': 0.873999092888352, 'support': 1297.0}, 'weighted avg': {'precision': 0.8746297485361414, 'recall': 0.874325366229761, 'f1-score': 0.8743896321909476, 'support': 1297.0}}, 'confusion_matrix': array([[534,  75],\n       [ 88, 600]])}\n\n--- SVM Model Testing ---\nBest Parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\nBest Cross-Validation Score: 0.7290716949854953\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.76      0.43      0.55       609\n           1       0.64      0.88      0.74       688\n\n    accuracy                           0.67      1297\n   macro avg       0.70      0.66      0.64      1297\nweighted avg       0.69      0.67      0.65      1297\n\n\n\n\n\n\n\n\n\n\n{'model': SVC(C=10, class_weight='balanced', gamma='auto', random_state=42), 'classification_report': {'0': {'precision': 0.7586206896551724, 'recall': 0.43349753694581283, 'f1-score': 0.5517241379310345, 'support': 609.0}, '1': {'precision': 0.6364594309799789, 'recall': 0.877906976744186, 'f1-score': 0.7379352474037875, 'support': 688.0}, 'accuracy': 0.669236700077101, 'macro avg': {'precision': 0.6975400603175756, 'recall': 0.6557022568449995, 'f1-score': 0.644829692667411, 'support': 1297.0}, 'weighted avg': {'precision': 0.6938196518999425, 'recall': 0.669236700077101, 'f1-score': 0.6505007326243684, 'support': 1297.0}}, 'confusion_matrix': array([[264, 345],\n       [ 84, 604]])}\n\n\n\nModel Evaluation Metrics\n\nBinary Classification Metrics\n• Accuracy: Measures the percentage of correctly classified samples across all classes.\n• Precision, Recall, F1-Score: Evaluates the model’s performance for each class, balancing false positives and false negatives.\n• Confusion Matrix: Used to visualize the distribution of predictions across classes.\n• Cross-Validation Score: Captures model consistency by evaluating performance across multiple data splits.\n\n\n\nResults\n\nKNN:\n• Best Cross-Validation Score: 0.939\nRandom Forest:\n• Best Cross-Validation Score: 0.965\nSVM:\n• Best Cross-Validation Score: 0.729\n\n\n\nImplications\n• Model Performance: The Random Forest and KNN models performed well, with accuracy scores of approximately 87% and 88%, respectively. These models also showed a good balance of precision and recall, especially for class 1 (donation), where recall was higher than precision, suggesting that the models are better at identifying suitable donation items.\n• Model Comparison: The SVM model, despite having a high recall for class 1, struggled with precision and overall accuracy. This suggests that SVM might be less suited for this particular classification task, especially if false positives are a significant concern.\n• Practical Application: A high-performing model can help in identifying food items that are eligible for donation, which can be leveraged in food recovery programs. The models can help reduce food waste by accurately predicting which food is fit for donation based on its nutritional and physical properties."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-selection-and-code",
    "href": "technical-details/supervised-learning/main.html#model-selection-and-code",
    "title": "Supervised Learning",
    "section": "Model Selection and Code",
    "text": "Model Selection and Code\nHere, we have built and evaluated three distinct types of machine learning models to address different aspects of food waste prediction and resource optimization. These models—multi-class classification, binary classification, and regression—each tackle unique challenges and offer insights into different dimensions of food waste management.\n\nMulti-Class Classification:\n\nThe first model is a multi-class classification task, where the goal is to predict the primary disposal method for different food items based on their physical characteristics and nutritional content. In this case, the target variable is the disposal method, which can take on multiple possible values (e.g., composting, recycling, or landfill). By leveraging features such as water footprint, nutritional composition (e.g., calories, protein, fat), and food categories, we aim to predict the most likely disposal method for each food item. This model is particularly useful in scenarios where the disposal method needs to be optimized based on the type of food waste, offering insights into how different food characteristics influence disposal choices.\n\nBinary Classification:\n\nThe second model is a binary classification task, which is designed to predict whether a given food item is fit for human consumption or not. This model uses various nutritional and environmental features as inputs, including surplus quantities, water footprint, and food characteristics. The target variable is binary: a food item is either classified as fit for consumption (0) or not fit for consumption (1). This binary classification task is valuable for food waste management systems where the goal is to separate consumable food from inedible or waste-bound food. The model’s output can guide decisions about redistribution, donation, or disposal, thus helping reduce food wastage by ensuring that edible food is appropriately utilized.\n\nRegression:\n\nThe third model is a regression task, which predicts the surplus value of food in US dollars based on various physical and nutritional features. Unlike classification, regression involves predicting a continuous variable—in this case, the monetary value of surplus food. The model utilizes features like food categories, nutritional content, and water footprint to estimate the financial surplus. This type of model is instrumental in helping organizations quantify the economic impact of food waste and identify opportunities for recovering value from surplus food. By accurately predicting the surplus value, the model can inform decisions around reselling, donating, or optimizing the food supply chain for maximum economic benefit.\n\nModel Rationale\nThe choice of models—K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machines (SVM)—was guided by their complementary strengths.\n• KNN: Selected for its simplicity and effectiveness in handling multiclass problems with non-linear decision boundaries.\n• Random Forest: Chosen for its robust performance across diverse datasets, ability to handle high-dimensional data, and built-in feature importance evaluation.\n• SVM: Included for its strength in finding optimal hyperplanes in high-dimensional spaces and its kernel flexibility for complex decision boundaries.\n\n\nOverview of Algorithms\n• KNN: Classifies samples by majority voting among k nearest neighbors in the feature space, using distance metrics like Manhattan or Euclidean.\n• Random Forest: Constructs multiple decision trees during training, averaging their outputs for classification to reduce overfitting and variance.\n• SVM: Uses a hyperplane to separate classes with maximum margin and can apply non-linear kernels such as RBF for complex classification tasks.\n\n\nSplit Methods\nThe dataset was divided into training and testing subsets using a train-test split, ensuring that the model’s performance generalizes to unseen data. Additionally, cross-validation (GridSearchCV) was used during hyperparameter tuning to assess the model’s stability and avoid overfitting.\n\n\nDataset Proportions\nThe dataset was split into 80% training data and 20% testing data to ensure adequate data for both model training and evaluation.\n\n\nPreprocessing Techniques\nNormalization\nA StandardScaler was applied to normalize the feature variables, ensuring that all features have zero mean and unit variance. This step was particularly critical for algorithms like SVM and KNN, which are sensitive to the scale of input data.\nBalancing the Dataset\nClass imbalances were addressed using SMOTE (Synthetic Minority Oversampling Technique) combined with ENN (Edited Nearest Neighbors) via SMOTEENN. This method oversamples the minority classes and removes noisy samples near class boundaries, ensuring balanced class representation in the training data.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nclass SupervisedLearning:\n    def __init__(self, data):\n        self.data = data\n        self.X_train = None\n        self.X_val = None\n        self.X_test = None\n        self.y_train = None\n        self.y_val = None\n        self.y_test = None\n\n    def prepare_data(self, input_features, target_feature, test_size=0.2, val_size=0.2, balance_data=True, cat_target = True):\n        # Drop rows with missing target or feature values\n        self.data = self.data.dropna(subset=input_features + [target_feature])\n    \n        # Extract features and target\n        X = self.data[input_features]\n        y = self.data[target_feature]\n\n    \n        if cat_target:\n            # Further split training+validation into training and validation sets\n            # Split into training+validation and test sets\n            X_train_val, X_test, y_train_val, y_test = train_test_split(\n                X, y, test_size=test_size, stratify=y, random_state=42\n            )\n\n            X_train, X_val, y_train, y_val = train_test_split(\n                X_train_val, y_train_val, test_size=val_size / (1 - test_size), stratify=y_train_val, random_state=42\n            )\n        else:\n            # Split into training+validation and test sets\n            X_train_val, X_test, y_train_val, y_test = train_test_split(\n                X, y, test_size=test_size, random_state=42\n            )\n            X_train, X_val, y_train, y_val = train_test_split(\n                X_train_val, y_train_val, test_size=val_size / (1 - test_size), random_state=42\n            )\n    \n        # Balance the data using SMOTEENN if specified\n        if balance_data:\n            class_counts = y_train.value_counts()\n            min_class_samples = class_counts.min()\n            k_neighbors = min(5, min_class_samples - 1)  # Ensure k_neighbors &lt; min_class_samples\n        \n            if k_neighbors &lt; 1:\n                print(\"Skipping SMOTEENN as one or more classes have fewer than 2 samples.\")\n            else:\n                smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n                smoteenn = SMOTEENN(random_state=42, smote=smote)\n                X_train, y_train = smoteenn.fit_resample(X_train, y_train)\n    \n        # Store the splits\n        self.X_train, self.X_val, self.X_test = X_train, X_val, X_test\n        self.y_train, self.y_val, self.y_test = y_train, y_val, y_test\n\n    def multiclass_classification(self, input_features, target_feature, model_type=\"knn\", use_scaling=True, **model_params):\n        # Standardize the features if required\n        if use_scaling:\n            scaler = StandardScaler()\n            self.X_train = scaler.fit_transform(self.X_train)\n            self.X_test = scaler.transform(self.X_test)\n            self.X_val = scaler.transform(self.X_val)\n\n        # Choose model and perform tuning (if applicable)\n        if model_type == \"knn\":\n            param_grid = {\n                'n_neighbors': [3, 5, 7, 10],\n                'weights': ['uniform', 'distance'],\n                'metric': ['euclidean', 'manhattan']\n            }\n            self.grid_search_model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, n_jobs=-1)\n\n        elif model_type == \"random_forest\":\n            param_grid = {\n                'n_estimators': [100, 200, 300],\n                'max_depth': [1,2,3, 5, 7, 10],\n                'min_samples_split': [2, 5],\n                'min_samples_leaf': [1, 2],\n                'max_features': ['sqrt', 'log2', None]\n            }\n            self.grid_search_model = GridSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42), param_grid, cv=5, n_jobs=-1)\n\n        elif model_type == \"svm\":\n            param_grid = {\n                'C': [0.1, 0.1, 10],\n                'kernel': ['linear', 'rbf'],\n                'gamma': ['scale', 'auto']\n            }\n            self.grid_search_model = GridSearchCV(SVC(class_weight='balanced', random_state=42), param_grid, cv=5, n_jobs=-1)\n\n        else:\n            raise ValueError(\"Unsupported model_type. Use 'knn', 'random_forest', or 'svm'.\")\n\n        # Fit the GridSearchCV model\n        self.grid_search_model.fit(self.X_train, self.y_train)\n\n        # Retrieve best estimator and parameters\n        self.model = self.grid_search_model.best_estimator_\n        print(\"Best Parameters:\", self.grid_search_model.best_params_)\n        print(\"Best Cross-Validation Score:\", self.grid_search_model.best_score_)\n\n        # Predictions\n        y_pred = self.model.predict(self.X_test)\n\n        # Evaluation\n        print(\"Classification Report:\\n\", classification_report(self.y_test, y_pred))\n        cm = confusion_matrix(self.y_test, y_pred)\n\n        # Plotting Confusion Matrix\n        fig, ax = plt.subplots(figsize=(12, 6))\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.model.classes_)\n        disp.plot(cmap='viridis', values_format='d', ax=ax)\n        plt.title(\"Confusion Matrix\")\n        plt.show()\n\n        # Feature importance plot (for models that support it)\n        if model_type == \"random_forest\":\n            feature_importances = self.model.feature_importances_\n            sns.barplot(x=feature_importances, y=input_features)\n            plt.title(\"Feature Importances\")\n            plt.show()\n\n        # Returning model and metrics for further analysis\n        return {\n            \"model\": self.model,\n            \"classification_report\": classification_report(self.y_test, y_pred, output_dict=True),\n            \"confusion_matrix\": cm\n        }\n    def one_hot_encode(self, column):\n        \"\"\"\n        One-hot encode a specified categorical column in the dataset.\n        \n        Parameters:\n            column (str): The name of the column to be one-hot encoded.\n        \"\"\"\n        if column not in self.data.columns:\n            raise ValueError(f\"Column '{column}' not found in the dataset.\")\n        \n        # Perform one-hot encoding\n        encoded_columns = pd.get_dummies(self.data[column], prefix=column)\n        \n        # Drop the original column and concatenate the new one-hot encoded columns\n        self.data = pd.concat([self.data.drop(columns=[column]), encoded_columns], axis=1)\n        print(f\"One-hot encoded column '{column}' and updated the dataset.\")\n        \n    def regression(self, input_features, target_feature, model_type=\"random_forest\", use_scaling=True, **model_params):\n        # Standardize the features if required\n        if use_scaling:\n            scaler = StandardScaler()\n            self.X_train = scaler.fit_transform(self.X_train)\n            self.X_test = scaler.transform(self.X_test)\n            self.X_val = scaler.transform(self.X_val)\n\n        # Choose model and define parameter grid\n        if model_type == \"random_forest\":\n            param_grid = {\n                'n_estimators': [100, 200, 300],\n                'max_depth': [3, 5, 10, 20],\n                'min_samples_split': [2, 5],\n                'min_samples_leaf': [1, 2],\n                'max_features': ['sqrt', 'log2', None]\n            }\n            self.grid_search_model = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, n_jobs=-1)\n\n        elif model_type == \"linear_regression\":\n            self.grid_search_model = LinearRegression()\n        \n        else:\n            raise ValueError(\"Unsupported model_type. Use 'random_forest' or 'linear_regression'.\")\n\n        # Fit the model\n        self.grid_search_model.fit(self.X_train, self.y_train)\n\n        # If GridSearchCV was used, get the best estimator\n        if model_type == \"random_forest\":\n            self.model = self.grid_search_model.best_estimator_\n            print(\"Best Parameters:\", self.grid_search_model.best_params_)\n            print(\"Best Cross-Validation Score:\", self.grid_search_model.best_score_)\n        else:\n            self.model = self.grid_search_model\n        \n        # Predictions\n        y_pred = self.model.predict(self.X_test)\n\n        # Evaluation Metrics\n        mse = mean_squared_error(self.y_test, y_pred)\n        mae = mean_absolute_error(self.y_test, y_pred)\n        r2 = r2_score(self.y_test, y_pred)\n\n        print(\"Mean Squared Error (MSE):\", mse)\n        print(\"Mean Absolute Error (MAE):\", mae)\n        print(\"R-squared (R2):\", r2)\n\n        # Returning model and metrics for further analysis\n        return {\n            \"model\": self.model,\n            \"mse\": mse,\n            \"mae\": mae,\n            \"r2\": r2\n        }\n    \n    def binary_classification(self, input_features, target_feature, model_type=\"random_forest\", use_scaling=True, **model_params):\n        # Standardize the features if required\n        if use_scaling:\n            scaler = StandardScaler()\n            self.X_train = scaler.fit_transform(self.X_train)\n            self.X_test = scaler.transform(self.X_test)\n            self.X_val = scaler.transform(self.X_val)\n\n        # Choose model and perform tuning (if applicable)\n        if model_type == \"knn\":\n            param_grid = {\n                'n_neighbors': [3, 5, 7, 10],\n                'weights': ['uniform', 'distance'],\n                'metric': ['euclidean', 'manhattan']\n            }\n            self.grid_search_model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, n_jobs=-1)\n\n        elif model_type == \"random_forest\":\n            param_grid = {\n                'n_estimators': [100, 200, 300],\n                'max_depth': [3, 5, 7, 10,15, 20],\n                'min_samples_split': [2, 5],\n                'min_samples_leaf': [1, 2],\n                'max_features': ['sqrt', 'log2', None]\n                }\n            self.grid_search_model = GridSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42), param_grid, cv=5, n_jobs=-1)\n\n        elif model_type == \"svm\":\n            param_grid = {\n                'C': [0.1, 0.5, 1, 10],\n                'kernel': ['linear', 'rbf'],\n                'gamma': ['scale', 'auto']\n            }\n            self.grid_search_model = GridSearchCV(SVC(class_weight='balanced', random_state=42), param_grid, cv=5, n_jobs=-1)\n\n        else:\n            raise ValueError(\"Unsupported model_type. Use 'knn', 'random_forest', or 'svm'.\")\n\n        # Fit the GridSearchCV model\n        self.grid_search_model.fit(self.X_train, self.y_train)\n\n        # Retrieve best estimator and parameters\n        self.model = self.grid_search_model.best_estimator_\n        print(\"Best Parameters:\", self.grid_search_model.best_params_)\n        print(\"Best Cross-Validation Score:\", self.grid_search_model.best_score_)\n\n        # Predictions\n        y_pred = self.model.predict(self.X_test)\n\n        # Evaluation\n        print(\"Classification Report:\\n\", classification_report(self.y_test, y_pred))\n        cm = confusion_matrix(self.y_test, y_pred)\n\n        # Plotting Confusion Matrix\n        fig, ax = plt.subplots(figsize=(12, 6))\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.model.classes_)\n        disp.plot(cmap='viridis', values_format='d', ax=ax)\n        plt.title(\"Confusion Matrix\")\n        plt.show()\n\n        # Returning model and metrics for further analysis\n        return {\n            \"model\": self.model,\n            \"classification_report\": classification_report(self.y_test, y_pred, output_dict=True),\n            \"confusion_matrix\": cm\n        }\n\n\ndata = pd.read_csv('../../data/processed-data/food_merged.csv')\n\ncategory_columns = [\n    'tons_donations', 'tons_industrial_uses', 'tons_animal_feed',\n    'tons_anaerobic_digestion', 'tons_composting', 'tons_not_harvested',\n    'tons_incineration', 'tons_land_application', 'tons_landfill',\n    'tons_sewer', 'tons_dumping'\n]\n\ndata['total'] = data[category_columns].sum(axis=1)\n\n# Calculate the percentage of total for each column\npercentage_df = data[category_columns].div(data['total'], axis=0)\n\n# Find the column with the maximum percentage for each row\ndata['disposal_method'] = percentage_df.idxmax(axis=1)\ndata['disposal_method'] = data['disposal_method'].str.replace('tons_', '', regex=False)\n\n# Drop the total column if no longer needed\ndata.drop(columns=['total'], inplace=True)\n\ndata['donation_bin'] = np.where(data['tons_donations'] &gt; 0, 1, 0)\nprint(data.shape)\n\n(7084, 42)\n\n\n/var/folders/sj/4yswlk7n08x8jfcfnyw2vkqw0000gn/T/ipykernel_89931/718609560.py:16: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n  data['disposal_method'] = percentage_df.idxmax(axis=1)"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#linear-regressuib",
    "href": "technical-details/supervised-learning/main.html#linear-regressuib",
    "title": "Supervised Learning",
    "section": "Linear Regressuib",
    "text": "Linear Regressuib\nThis analysis involves testing two regression models (Random Forest and Linear Regression) to predict the us_dollars_surplus of food items. The dataset includes features such as nutritional content and water footprint, along with one-hot encoded values for the food_category column. The goal is to predict the surplus value (in dollars) based on these characteristics, and the models’ performance is assessed using common regression metrics.\nThe input features used for prediction include various food-related characteristics, such as:\n• Environmental Impact Factor: gallons_water_footprint\n• Nutritional Composition: serving_size, calories, protein, fat, carbs, fiber, calcium, iron, sodium\n• One Hot Encoded Food Category: food_category\n\nModel Evaluation Metrics:\nThe evaluation metrics provide a way to assess how well each model performs on the regression task. The primary metrics used here are:\n• Mean Squared Error (MSE): This metric quantifies the average squared difference between the actual and predicted values. A lower MSE indicates a better fit of the model to the data.\n• Mean Absolute Error (MAE): This metric gives the average of the absolute differences between predicted and actual values. Like MSE, a lower MAE is preferred, but it is less sensitive to outliers compared to MSE.\n• R-squared (R²): This metric indicates the proportion of the variance in the target variable that is predictable from the independent variables. An R² value closer to 1 means the model explains most of the variance in the data, while a value closer to 0 suggests a poor fit.\n\n# Instantiate the class\nsl = SupervisedLearning(data)\n\n# One-hot encode the 'food_category' column\nsl.one_hot_encode('food_category')\n\n# Define input and target features\ninput_features = [\n    'gallons_water_footprint', 'protein', 'fat', 'carbs', 'calories', 'fiber',\n    'calcium', 'iron', 'sodium'\n] + [col for col in sl.data.columns if col.startswith('food_category_')]  \ntarget_feature = 'us_dollars_surplus'\n\n# Prepare the data\nsl.prepare_data(input_features, target_feature, cat_target = False)\n\n# Perform regression\nprint(\"\\n--- Random Forest Model Testing ---\")\n\nresults_rf = sl.regression(input_features, target_feature, model_type=\"random_forest\", use_scaling=True)\nprint(results_rf)\n\n# Perform regression\nprint(\"\\n--- Linear Reggression Model Testing ---\")\n\nresults_lr = sl.regression(input_features, target_feature, model_type=\"linear_regression\", use_scaling=True)\nprint(results_lr)\n\nOne-hot encoded column 'food_category' and updated the dataset.\nSkipping SMOTEENN as one or more classes have fewer than 2 samples.\n\n--- Random Forest Model Testing ---\n\n\n/Users/shanaywadhwani/Desktop/anaconda3/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n  _data = np.array(data, dtype=dtype, copy=copy,\n\n\nBest Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\nBest Cross-Validation Score: 0.9692753382410217\nMean Squared Error (MSE): 1.0930396794092634e+16\nMean Absolute Error (MAE): 54092284.88337758\nR-squared (R2): 0.9679093204991429\n{'model': RandomForestRegressor(max_depth=20, max_features='sqrt', random_state=42), 'mse': 1.0930396794092634e+16, 'mae': 54092284.88337758, 'r2': 0.9679093204991429}\n\n--- Linear Reggression Model Testing ---\nMean Squared Error (MSE): 8.536678751781533e+16\nMean Absolute Error (MAE): 156127623.3713102\nR-squared (R2): 0.7493706523323531\n{'model': LinearRegression(), 'mse': 8.536678751781533e+16, 'mae': 156127623.3713102, 'r2': 0.7493706523323531}\n\n\n\n\nResults\nRandom Forest Model:\nThe Random Forest model’s high R² value (96.79%) and relatively low error metrics (MSE and MAE) suggest that it is well-suited for predicting us_dollars_surplus in this context. The model’s ability to capture complex, non-linear relationships between the input features (such as nutritional content, water footprint, and food category) makes it a powerful tool for this regression task. The strong performance of the Random Forest model implies that the factors influencing food surplus are likely complex and interdependent, and a tree-based approach that considers interactions between variables is optimal.\nImplications for Decision-Making:\n• This model can be trusted to provide relatively accurate estimates of food surplus, which is crucial for resource management, sustainability efforts, and forecasting food waste reduction strategies.\n• The high R² also suggests that predictive decisions (e.g., optimizing food donations based on surplus values) can be made with a high degree of confidence.\nLinear Regression Model:\nThe Linear Regression model, with an R² of 74.94%, performs substantially worse than Random Forest. This indicates that the relationships between the features and the target variable may not be well-represented by a simple linear model. Although Linear Regression is often favored for its interpretability and simplicity, its performance in this case implies that the problem might require more sophisticated models to capture the intricate interactions between variables.\nImplications for Decision-Making:\n• While Linear Regression could still be used in scenarios requiring fast and interpretable predictions, it might lead to less reliable decisions in complex situations where non-linearities and interactions between features are critical.\n• The model’s higher error metrics highlight the need for further refinement, such as feature engineering or incorporating more sophisticated modeling techniques, to improve accuracy in food surplus predictions.\n\n\nImplications\n• Predicting Food Surplus: Accurate predictions of food surplus can significantly impact food redistribution strategies, enabling organizations to allocate resources more effectively, reduce food waste, and improve sustainability practices.\n• This could lead to more efficient use of food resources, helping both to minimize waste and address hunger by redirecting surplus food to charitable organizations.\n• Improved Policy and Operational Decision-Making: Decision-makers can use the insights derived from the Random Forest model to better plan for food production and distribution, ensuring that surplus food is identified early and directed to appropriate channels. This can also aid in developing policies that prioritize food sustainability and efficient use of resources."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-performance-summary",
    "href": "technical-details/supervised-learning/main.html#model-performance-summary",
    "title": "Supervised Learning",
    "section": "Model Performance Summary",
    "text": "Model Performance Summary\n• Improved Waste Management: By leveraging these predictive models, we can optimize food waste management strategies, predicting the disposal methods most likely to be employed based on food characteristics. This could help businesses or organizations implement more efficient waste reduction practices and optimize recycling or composting initiatives.\n• Environmental Impact: Predicting the surplus value and disposal method for food items can have substantial environmental implications. The models can provide insights into where food waste could be better managed (e.g., through donation, composting, or recycling), which would help reduce environmental harm by diverting waste from landfills. Furthermore, aligning food waste strategies with sustainability goals can contribute to a reduction in water, energy, and other resource consumption related to food production.\n• Economic Value: The regression model predicting the surplus value in US dollars offers valuable insights for food supply chain optimization. By accurately forecasting the financial surplus related to food waste, businesses can make more informed decisions, potentially recovering value from surplus food, such as donating it, reselling it, or repurposing it for other uses.\n• Behavioral Insights: With accurate predictions of food disposal methods and their economic impact, businesses can start to identify patterns in consumer behavior and food supply. This data could be used to devise better strategies to reduce food waste at various stages of the food supply chain—growing, processing, packaging, and consumption.\nBy focusing on continuous improvement and optimization of these models, there is potential to create a system that not only predicts food waste and disposal methods accurately but also contributes to long-term sustainability, waste reduction, and economic efficiency."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#future-steps",
    "href": "technical-details/supervised-learning/main.html#future-steps",
    "title": "Supervised Learning",
    "section": "Future Steps",
    "text": "Future Steps\n• Model Optimization: The current models (KNN, Random Forest, and Linear Regression) have demonstrated solid performance, but future steps should focus on further optimizing these models. For instance, hyperparameter tuning using GridSearchCV can be continued to refine model parameters, especially for Random Forest and SVM. Additionally, exploring more complex algorithms like Gradient Boosting or Neural Networks may lead to better performance, especially if the dataset grows in size.\n• Feature Engineering: While the feature set used in the current models covers a broad range of nutritional and environmental factors, additional features could enhance the model’s predictive power. Investigating domain-specific features, such as food waste factors, consumer behavior, or additional environmental impacts, could provide a more holistic view of food disposal decisions. This might also involve combining features through transformations (e.g., interaction terms or polynomial features) to capture more intricate relationships.\n• Data Quality and Expansion: The current models rely on a rich set of features, but improving data quality is always a continuous process. Handling outliers more robustly, filling missing values more effectively, and ensuring that categorical data is properly encoded (like the one-hot encoding of food categories) will help refine the models. Furthermore, expanding the dataset to include more diverse food categories or a broader geographical representation would help improve model generalization.\n• Evaluation and Validation: Future work should focus on comprehensive evaluation metrics, beyond just accuracy, by including precision, recall, F1-score, and ROC-AUC in more detail to better handle class imbalance. This is especially relevant when predicting food waste, as the consequences of misclassification can be significant. Incorporating a more robust validation strategy, such as time-series cross-validation or k-fold cross-validation, may also help assess model stability and performance across different subsets of the data. • Model Optimization: The current models (KNN, Random Forest, and Linear Regression) have demonstrated solid performance, but future steps should focus on further optimizing these models. For instance, hyperparameter tuning using GridSearchCV can be continued to refine model parameters, especially for Random Forest and SVM. Additionally, exploring more complex algorithms like Gradient Boosting or Neural Networks may lead to better performance, especially if the dataset grows in size.\n• Feature Engineering: While the feature set used in the current models covers a broad range of nutritional and environmental factors, additional features could enhance the model’s predictive power. Investigating domain-specific features, such as food waste factors, consumer behavior, or additional environmental impacts, could provide a more holistic view of food disposal decisions. This might also involve combining features through transformations (e.g., interaction terms or polynomial features) to capture more intricate relationships.\n• Data Quality and Expansion: The current models rely on a rich set of features, but improving data quality is always a continuous process. Handling outliers more robustly, filling missing values more effectively, and ensuring that categorical data is properly encoded (like the one-hot encoding of food categories) will help refine the models. Furthermore, expanding the dataset to include more diverse food categories or a broader geographical representation would help improve model generalization.\n• Evaluation and Validation: Future work should focus on comprehensive evaluation metrics, beyond just accuracy, by including precision, recall, F1-score, and ROC-AUC in more detail to better handle class imbalance. This is especially relevant when predicting food waste, as the consequences of misclassification can be significant. Incorporating a more robust validation strategy, such as time-series cross-validation or k-fold cross-validation, may also help assess model stability and performance across different subsets of the data."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#linear-reggression",
    "href": "technical-details/supervised-learning/main.html#linear-reggression",
    "title": "Supervised Learning",
    "section": "Linear Reggression",
    "text": "Linear Reggression\nThis analysis involves testing two regression models (Random Forest and Linear Regression) to predict the us_dollars_surplus of food items. The dataset includes features such as nutritional content and water footprint, along with one-hot encoded values for the food_category column. The goal is to predict the surplus value (in dollars) based on these characteristics, and the models’ performance is assessed using common regression metrics.\nThe input features used for prediction include various food-related characteristics, such as:\n• Environmental Impact Factor: gallons_water_footprint\n• Nutritional Composition: serving_size, calories, protein, fat, carbs, fiber, calcium, iron, sodium\n• One Hot Encoded Food Category: food_category\n\nModel Evaluation Metrics:\nThe evaluation metrics provide a way to assess how well each model performs on the regression task. The primary metrics used here are:\n• Mean Squared Error (MSE): This metric quantifies the average squared difference between the actual and predicted values. A lower MSE indicates a better fit of the model to the data.\n• Mean Absolute Error (MAE): This metric gives the average of the absolute differences between predicted and actual values. Like MSE, a lower MAE is preferred, but it is less sensitive to outliers compared to MSE.\n• R-squared (R²): This metric indicates the proportion of the variance in the target variable that is predictable from the independent variables. An R² value closer to 1 means the model explains most of the variance in the data, while a value closer to 0 suggests a poor fit.\n\n# Instantiate the class\nsl = SupervisedLearning(data)\n\n# One-hot encode the 'food_category' column\nsl.one_hot_encode('food_category')\n\n# Define input and target features\ninput_features = [\n    'gallons_water_footprint', 'protein', 'fat', 'carbs', 'calories', 'fiber',\n    'calcium', 'iron', 'sodium'\n] + [col for col in sl.data.columns if col.startswith('food_category_')]  \ntarget_feature = 'us_dollars_surplus'\n\n# Prepare the data\nsl.prepare_data(input_features, target_feature, cat_target = False)\n\n# Perform regression\nprint(\"\\n--- Random Forest Model Testing ---\")\n\nresults_rf = sl.regression(input_features, target_feature, model_type=\"random_forest\", use_scaling=True)\nprint(results_rf)\n\n# Perform regression\nprint(\"\\n--- Linear Reggression Model Testing ---\")\n\nresults_lr = sl.regression(input_features, target_feature, model_type=\"linear_regression\", use_scaling=True)\nprint(results_lr)\n\nOne-hot encoded column 'food_category' and updated the dataset.\nSkipping SMOTEENN as one or more classes have fewer than 2 samples.\n\n--- Random Forest Model Testing ---\n\n\n/Users/shanaywadhwani/Desktop/anaconda3/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n  _data = np.array(data, dtype=dtype, copy=copy,\n\n\nBest Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\nBest Cross-Validation Score: 0.9692753382410217\nMean Squared Error (MSE): 1.0930396794092634e+16\nMean Absolute Error (MAE): 54092284.88337758\nR-squared (R2): 0.9679093204991429\n{'model': RandomForestRegressor(max_depth=20, max_features='sqrt', random_state=42), 'mse': 1.0930396794092634e+16, 'mae': 54092284.88337758, 'r2': 0.9679093204991429}\n\n--- Linear Reggression Model Testing ---\nMean Squared Error (MSE): 8.536678751781533e+16\nMean Absolute Error (MAE): 156127623.3713102\nR-squared (R2): 0.7493706523323531\n{'model': LinearRegression(), 'mse': 8.536678751781533e+16, 'mae': 156127623.3713102, 'r2': 0.7493706523323531}\n\n\n\n\nResults\nRandom Forest Model:\nThe Random Forest model’s high R² value (96.79%) and relatively low error metrics (MSE and MAE) suggest that it is well-suited for predicting us_dollars_surplus in this context. The model’s ability to capture complex, non-linear relationships between the input features (such as nutritional content, water footprint, and food category) makes it a powerful tool for this regression task. The strong performance of the Random Forest model implies that the factors influencing food surplus are likely complex and interdependent, and a tree-based approach that considers interactions between variables is optimal.\nImplications for Decision-Making:\n• This model can be trusted to provide relatively accurate estimates of food surplus, which is crucial for resource management, sustainability efforts, and forecasting food waste reduction strategies.\n• The high R² also suggests that predictive decisions (e.g., optimizing food donations based on surplus values) can be made with a high degree of confidence.\nLinear Regression Model:\nThe Linear Regression model, with an R² of 74.94%, performs substantially worse than Random Forest. This indicates that the relationships between the features and the target variable may not be well-represented by a simple linear model. Although Linear Regression is often favored for its interpretability and simplicity, its performance in this case implies that the problem might require more sophisticated models to capture the intricate interactions between variables.\nImplications for Decision-Making:\n• While Linear Regression could still be used in scenarios requiring fast and interpretable predictions, it might lead to less reliable decisions in complex situations where non-linearities and interactions between features are critical.\n• The model’s higher error metrics highlight the need for further refinement, such as feature engineering or incorporating more sophisticated modeling techniques, to improve accuracy in food surplus predictions.\n\n\nImplications\n• Predicting Food Surplus: Accurate predictions of food surplus can significantly impact food redistribution strategies, enabling organizations to allocate resources more effectively, reduce food waste, and improve sustainability practices.\n• This could lead to more efficient use of food resources, helping both to minimize waste and address hunger by redirecting surplus food to charitable organizations.\n• Improved Policy and Operational Decision-Making: Decision-makers can use the insights derived from the Random Forest model to better plan for food production and distribution, ensuring that surplus food is identified early and directed to appropriate channels. This can also aid in developing policies that prioritize food sustainability and efficient use of resources."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#summary",
    "href": "technical-details/supervised-learning/main.html#summary",
    "title": "Supervised Learning",
    "section": "Summary",
    "text": "Summary\n• Improved Waste Management: By leveraging these predictive models, we can optimize food waste management strategies, predicting the disposal methods most likely to be employed based on food characteristics. This could help businesses or organizations implement more efficient waste reduction practices and optimize recycling or composting initiatives.\n• Environmental Impact: Predicting the surplus value and disposal method for food items can have substantial environmental implications. The models can provide insights into where food waste could be better managed (e.g., through donation, composting, or recycling), which would help reduce environmental harm by diverting waste from landfills. Furthermore, aligning food waste strategies with sustainability goals can contribute to a reduction in water, energy, and other resource consumption related to food production.\n• Economic Value: The regression model predicting the surplus value in US dollars offers valuable insights for food supply chain optimization. By accurately forecasting the financial surplus related to food waste, businesses can make more informed decisions, potentially recovering value from surplus food, such as donating it, reselling it, or repurposing it for other uses.\n• Behavioral Insights: With accurate predictions of food disposal methods and their economic impact, businesses can start to identify patterns in consumer behavior and food supply. This data could be used to devise better strategies to reduce food waste at various stages of the food supply chain—growing, processing, packaging, and consumption.\nBy focusing on continuous improvement and optimization of these models, there is potential to create a system that not only predicts food waste and disposal methods accurately but also contributes to long-term sustainability, waste reduction, and economic efficiency."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#model-selection-and-code",
    "href": "technical-details/unsupervised-learning/main.html#model-selection-and-code",
    "title": "Unsupervised Learning",
    "section": "Model Selection and Code",
    "text": "Model Selection and Code\nThe goal is to classify food donations, predict dollar surplus from surplus food, and understand primary disposal methods. To achieve this, this section employs several unsupervised learning techniques to uncover patterns and insights from the data, which is structured based on various food-related attributes (e.g., nutritional values, waste amounts, water footprint).\nKey Techniques Used for Model Selection:\n\nPreprocessing:- Before applying any machine learning techniques, the data undergoes thorough preprocessing to ensure its suitability for analysis:\n• Handling Missing Values: The project uses imputation strategies to fill missing data, ensuring the integrity of the dataset. The imputer applies strategies median imputation.\n• Normalization: The numeric columns are normalized using standard or min-max scaling to ensure that all variables contribute equally during clustering and dimensionality reduction. After applying z-score scaling and min-max scaling it was evident that the untransformed data was better for clustering than the normalized data.\nDimensionality Reduction:\n• PCA (Principal Component Analysis): PCA is used for reducing the dimensionality of the dataset while retaining as much variance as possible. This helps in visualizing the data and understanding the primary sources of variability. The explained variance plot provides an insight into the number of components needed to represent the dataset effectively.\n• t-SNE (t-Distributed Stochastic Neighbor Embedding): t-SNE is applied to visualize the data in two or three dimensions, helping to identify potential clusters or patterns. This technique is particularly useful when dealing with high-dimensional data.\nClustering:\n• K-Means Clustering: The K-Means algorithm is applied to group the data into a predefined number of clusters. This method is chosen due to its simplicity and efficiency. The silhouette score, which measures how well-defined the clusters are, helps in assessing the quality of the clusters. The results are visualized using PCA-reduced data.\n• DBSCAN (Density-Based Spatial Clustering of Applications with Noise): DBSCAN is chosen for its ability to identify clusters of varying shapes and handle noise effectively. This is especially relevant for datasets where there might be outliers or regions of differing density. Silhouette scores are also computed to assess clustering quality.\n• Hierarchical Clustering: Hierarchical clustering, using methods like “ward” or “average,” is applied to create a dendrogram. This technique is valuable for understanding the hierarchical relationships between data points and visualizing how clusters are formed at different distance thresholds.\nEvaluation Metrics:\n• Silhouette Score: To evaluate the clustering results, the silhouette score is used. This score measures how similar each point is to its own cluster compared to other clusters. A higher silhouette score indicates well-defined and meaningful clusters. This metric is calculated for K-Means, DBSCAN, and hierarchical clustering.\n• Cluster Visualization: After applying each clustering method, the results are visualized to help understand the distribution of data points and the validity of the identified clusters. Visualizations using PCA or t-SNE reduce the data to two or three dimensions for easier interpretation.\n\nBy applying these unsupervised learning techniques, the project aims to uncover hidden patterns in food waste, such as identifying which food categories are most often wasted, understanding the relationships between nutritional composition and waste, and classifying surplus food into clusters that may correlate with its economic value or environmental impact. The combination of dimensionality reduction, clustering, and silhouette-based evaluation ensures that the analysis is both comprehensive and interpretable, making it possible to derive actionable insights for smarter food waste management practices.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n\nclass UnsupervisedLearning:\n    def __init__(self, data):\n        \"\"\"\n        Initialize the UnsupervisedLearning class with the dataset.\n        \n        Parameters:\n            data (pd.DataFrame): Input dataset for analysis.\n        \"\"\"\n        self.data = data\n        self.pca_result = None\n        self.tsne_result = None\n        self.clusters = {}\n    def preprocess_data(self, impute_strategy='mean', drop_na_threshold=0.5, normalize=True, scaler_type='standard'):\n        \"\"\"\n        Preprocess the data to ensure it's suitable for unsupervised learning:\n        - Drop columns with excessive missing values.\n        - Impute missing values for numeric columns.\n        - Convert non-numeric columns to numeric using encoding.\n        - Normalize numeric columns if required.\n\n        Parameters:\n            impute_strategy (str): Strategy for imputing missing values ('mean', 'median', 'most_frequent').\n            drop_na_threshold (float): Threshold for dropping columns with missing values (default: 0.5).\n            normalize (bool): Whether to normalize numeric columns (default: True).\n            scaler_type (str): Type of scaler to use ('standard' for StandardScaler, 'minmax' for MinMaxScaler).\n        \"\"\"\n        # Drop columns with excessive missing values\n        missing_ratio = self.data.isna().mean()\n        cols_to_drop = missing_ratio[missing_ratio &gt; drop_na_threshold].index\n        self.data.drop(columns=cols_to_drop, inplace=True)\n\n        # Separate numeric and non-numeric columns\n        numeric_cols = self.data.select_dtypes(include=['number']).columns\n        non_numeric_cols = self.data.select_dtypes(exclude=['number']).columns\n\n        # Handle non-numeric columns\n        if len(non_numeric_cols) &gt; 0:\n            self.data = pd.get_dummies(self.data, columns=non_numeric_cols, drop_first=True)\n\n        # Impute missing values in numeric columns\n        imputer = SimpleImputer(strategy=impute_strategy)\n        self.data[numeric_cols] = imputer.fit_transform(self.data[numeric_cols])\n\n        # Normalize numeric columns if required\n        if normalize:\n            scaler = StandardScaler() if scaler_type == 'standard' else MinMaxScaler()\n            self.data[numeric_cols] = scaler.fit_transform(self.data[numeric_cols])\n\n    def apply_pca(self, n_components=None):\n        \"\"\"\n        Apply PCA to the dataset and visualize the explained variance ratio.\n\n        Parameters:\n            n_components (int, optional): Number of components for PCA. \n                                          If None, all components are considered.\n        \"\"\"\n        pca = PCA(n_components=n_components)\n        pca_result = pca.fit_transform(self.data)\n        self.pca_result = pd.DataFrame(pca_result, columns=[f'PC{i+1}' for i in range(pca_result.shape[1])])\n        \n        # Explained variance plot\n        plt.figure(figsize=(10, 6))\n        plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o')\n        plt.title('Cumulative Explained Variance by PCA Components')\n        plt.xlabel('Number of Components')\n        plt.ylabel('Cumulative Explained Variance')\n        plt.grid()\n        plt.show()\n\n    def visualize_pca(self):\n        \"\"\"Visualize PCA-reduced data if 2 or 3 components are present.\"\"\"\n        if self.pca_result is None:\n            print(\"Please run `apply_pca()` first.\")\n            return\n        \n        if self.pca_result.shape[1] &gt;= 2:\n            plt.figure(figsize=(10, 6))\n            sns.scatterplot(data=self.pca_result, x='PC1', y='PC2', alpha=0.7)\n            plt.title('PCA Visualization (First Two Components)')\n            plt.xlabel('PC1')\n            plt.ylabel('PC2')\n            plt.grid()\n            plt.show()\n        else:\n            print(\"PCA visualization requires at least 2 components.\")\n\n    def apply_tsne(self, perplexity=30, n_components=2, random_state=42):\n        \"\"\"\n        Apply t-SNE to the dataset and visualize the results.\n\n        Parameters:\n            perplexity (int): Perplexity parameter for t-SNE.\n            n_components (int): Number of dimensions for t-SNE.\n            random_state (int): Random state for reproducibility.\n        \"\"\"\n        tsne = TSNE(n_components=n_components, perplexity=perplexity, random_state=random_state)\n        tsne_result = tsne.fit_transform(self.data)\n        self.tsne_result = pd.DataFrame(tsne_result, columns=[f'Dim{i+1}' for i in range(n_components)])\n        \n        # t-SNE plot\n        plt.figure(figsize=(10, 6))\n        sns.scatterplot(x=self.tsne_result['Dim1'], y=self.tsne_result['Dim2'], alpha=0.7)\n        plt.title(f't-SNE Visualization (Perplexity={perplexity})')\n        plt.xlabel('Dim1')\n        plt.ylabel('Dim2')\n        plt.grid()\n        plt.show()\n\n    def kmeans_clustering(self, n_clusters):\n        \"\"\"\n        Apply K-Means clustering and visualize clusters.\n\n        Parameters:\n            n_clusters (int): Number of clusters for K-Means.\n        \"\"\"\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        labels = kmeans.fit_predict(self.data)\n        self.clusters['kmeans'] = labels\n\n        silhouette = silhouette_score(self.data, labels)\n        print(f\"K-Means Silhouette Score: {silhouette:.4f}\")\n        \n        # Cluster visualization\n        plt.figure(figsize=(10, 6))\n        sns.scatterplot(x=self.pca_result['PC1'], y=self.pca_result['PC2'], hue=labels, palette='tab10', alpha=0.7)\n        plt.title('K-Means Clustering (PCA Reduced Data)')\n        plt.xlabel('PC1')\n        plt.ylabel('PC2')\n        plt.legend(title='Cluster')\n        plt.grid()\n        plt.show()\n    \n    def kmeans_clustering_tsne(self, n_clusters, palette='tab10'):\n        \"\"\"\n        Apply K-Means clustering and visualize clusters.\n\n        Parameters:\n            n_clusters (int): Number of clusters for K-Means.\n        \"\"\"\n        if not hasattr(self, 'tsne_result') or self.tsne_result.empty:\n            raise ValueError(\"t-SNE results not found. Run apply_tsne() before clustering.\")\n    \n        # Apply K-Means clustering\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        labels = kmeans.fit_predict(self.data)\n        self.clusters['kmeans'] = labels\n\n        # Calculate silhouette score\n        silhouette = silhouette_score(self.data, labels)\n        print(f\"K-Means Silhouette Score: {silhouette:.4f}\")\n\n        # Visualization\n        plt.figure(figsize=(10, 6))\n        sns.scatterplot(\n            x=self.tsne_result['Dim1'],\n            y=self.tsne_result['Dim2'],\n            hue=labels,\n            palette=palette,\n            alpha=0.7,\n            legend='full'\n        )\n        plt.title('K-Means Clustering (t-SNE Reduced Data)')\n        plt.xlabel('Dim1')\n        plt.ylabel('Dim2')\n        plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n        plt.grid()\n        plt.show()\n\n        return silhouette\n\n    def dbscan_clustering(self, eps, min_samples):\n        \"\"\"\n        Apply DBSCAN clustering and visualize clusters.\n\n        Parameters:\n            eps (float): The maximum distance between two samples for them to be considered as in the same neighborhood.\n            min_samples (int): The number of samples in a neighborhood for a point to be considered as a core point.\n        \"\"\"\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        labels = dbscan.fit_predict(self.data)\n        self.clusters['dbscan'] = labels\n\n        silhouette = silhouette_score(self.data, labels) if len(set(labels)) &gt; 1 else \"N/A\"\n        print(f\"DBSCAN Silhouette Score: {silhouette}\")\n        \n        # Cluster visualization\n        plt.figure(figsize=(10, 6))\n        sns.scatterplot(x=self.pca_result['PC1'], y=self.pca_result['PC2'], hue=labels, palette='tab10', alpha=0.7)\n        plt.title('DBSCAN Clustering (PCA Reduced Data)')\n        plt.xlabel('PC1')\n        plt.ylabel('PC2')\n        plt.legend(title='Cluster')\n        plt.grid()\n        plt.show()\n\n    def hierarchical_clustering(self, method='ward', distance_threshold=0):\n        \"\"\"\n        Apply Hierarchical clustering and visualize dendrogram.\n\n        Parameters:\n            method (str): Linkage criterion (e.g., 'ward', 'complete', 'average').\n            distance_threshold (float): Threshold for cutting the dendrogram.\n        \"\"\"\n        linkage_matrix = linkage(self.data, method=method)\n        plt.figure(figsize=(12, 8))\n        dendrogram(linkage_matrix, truncate_mode='level', p=5)\n        plt.title('Hierarchical Clustering Dendrogram')\n        plt.xlabel('Sample Index')\n        plt.ylabel('Distance')\n        plt.show()\n\n        if distance_threshold &gt; 0:\n            labels = fcluster(linkage_matrix, t=distance_threshold, criterion='distance')\n            self.clusters['hierarchical'] = labels\n            silhouette = silhouette_score(self.data, labels)\n            print(f\"Hierarchical Clustering Silhouette Score: {silhouette:.4f}\")\n\n\ndata = pd.read_csv('../../data/processed-data/food_merged.csv')\n\ncategory_columns = [\n    'tons_donations', 'tons_industrial_uses', 'tons_animal_feed',\n    'tons_anaerobic_digestion', 'tons_composting', 'tons_not_harvested',\n    'tons_incineration', 'tons_land_application', 'tons_landfill',\n    'tons_sewer', 'tons_dumping'\n]\n\ndata['total'] = data[category_columns].sum(axis=1)\n\n# Calculate the percentage of total for each column\npercentage_df = data[category_columns].div(data['total'], axis=0)\n\n# Find the column with the maximum percentage for each row\ndata['disposal_method'] = percentage_df.idxmax(axis=1)\ndata['disposal_method'] = data['disposal_method'].str.replace('tons_', '', regex=False)\n\n# Drop the total column if no longer needed\ndata.drop(columns=['total'], inplace=True)\n\ndata['donation_bin'] = np.where(data['tons_donations'] &gt; 0, 1, 0)\nprint(data.shape)\n\n(7084, 42)\n\n\n/var/folders/sj/4yswlk7n08x8jfcfnyw2vkqw0000gn/T/ipykernel_93714/718609560.py:16: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n  data['disposal_method'] = percentage_df.idxmax(axis=1)\n\n\n\nunsupervised_learner = UnsupervisedLearning(data)\nunsupervised_learner.preprocess_data(impute_strategy='median', normalize=False, scaler_type='standard')\n# Apply PCA\nunsupervised_learner.apply_pca(n_components=2)\nunsupervised_learner.visualize_pca()"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#pca",
    "href": "technical-details/unsupervised-learning/main.html#pca",
    "title": "Unsupervised Learning",
    "section": "PCA",
    "text": "PCA\nPCA is a linear technique that aims to maximize the variance of the data across the principal components. It does so by finding the directions in which the data varies the most and projecting the data onto these directions. While PCA effectively captures global structure and preserves variance in the data, it is not necessarily the best at preserving local relationships or non-linear structures in the data. For example, if the data lies on a non-linear manifold (e.g., a circle or spiral), PCA may not be able to capture this structure well. However, PCA is computationally efficient, interpretable, and works well when the data is linearly separable or when the goal is to reduce dimensionality while retaining as much variance as possible.\n• First Plot: The first two principal components capture 99.994% of the variance in the dataset, indicating that these components effectively represent the majority of the data’s variation. This suggests that the dataset’s structure is well captured in these two dimensions.\n• Second Plot: This plot visualizes the higher-dimensional data in a 2D space. It reveals that many of the data points are similar, but a significant portion of the dataset is spread across the first two principal components. This distribution suggests the potential for clustering within the data, as there may be groups of points that are more tightly packed together.\n\nunsupervised_learner.apply_tsne(perplexity=15)\nunsupervised_learner.apply_tsne(perplexity=30)\nunsupervised_learner.apply_tsne(perplexity=60)"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#t-sne",
    "href": "technical-details/unsupervised-learning/main.html#t-sne",
    "title": "Unsupervised Learning",
    "section": "t-SNE",
    "text": "t-SNE\nt-SNE is a non-linear technique designed specifically to preserve local structure by minimizing the divergence between probability distributions over pairwise similarities in the high-dimensional and low-dimensional spaces. It is particularly effective for visualizing data that has a complex, non-linear structure, as it tends to group similar data points together in the low-dimensional space. However, t-SNE has some limitations, such as its computational complexity, sensitivity to hyperparameters (like perplexity), and tendency to distort global structure in favor of local relationships. This makes t-SNE less suitable for tasks where preserving the global relationships between clusters or outliers is important.\nThe following three plots demonstrate the effect of varying the perplexity parameter on t-SNE dimensionality reduction:\n\nPerplexity = 15: At this setting, the t-SNE output shows more spread-out data points, with clusters being less discernible. The reduced dimensionality does not reveal clear groupings within the data.\nPerplexity = 30: With a perplexity of 30, the t-SNE output is less spread out compared to the previous plot, but still does not reveal any separable clusters. The data points appear more concentrated, yet distinct groupings are still not visible.\nPerplexity = 60: At a higher perplexity value, the t-SNE output becomes even more compressed, with the data points appearing inseparable. The points are tightly packed, making it challenging to identify meaningful clusters in the reduced dimensional space.\n\n\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef elbow_method_kmeans(data, max_clusters=10):\n    inertia = []\n    for n_clusters in range(1, max_clusters + 1):\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        kmeans.fit(data)\n        inertia.append(kmeans.inertia_)\n    \n    # Plot inertia vs. number of clusters\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, max_clusters + 1), inertia, marker='o')\n    plt.title('Elbow Method for Optimal Number of Clusters')\n    plt.xlabel('Number of Clusters')\n    plt.ylabel('Inertia')\n    plt.grid(True)\n    plt.show()\n\n# Example usage:\nelbow_method_kmeans(unsupervised_learner.data)\n\nunsupervised_learner.kmeans_clustering(n_clusters=3)\n\n\n\n\n\n\n\n\nK-Means Silhouette Score: 0.8908"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#k-means",
    "href": "technical-details/unsupervised-learning/main.html#k-means",
    "title": "Unsupervised Learning",
    "section": "K-Means",
    "text": "K-Means\nK-Means clustering is one of the most widely used unsupervised learning algorithms for partitioning data into distinct groups. The algorithm works by initializing a set of K centroids and iteratively refining them by assigning each data point to the nearest centroid. After assigning points to centroids, the centroids are updated to be the mean of the points in each cluster. This process repeats until convergence, where the centroids no longer change significantly. The K-Means algorithm is computationally efficient and performs well when clusters are spherical and relatively well-separated.\nIn our analysis, K-Means was applied to the PCA-reduced data, where the elbow method indicated that 3 clusters was the optimal choice. This choice was confirmed by the high silhouette score of 0.8908, which indicates that the clusters were well-separated and cohesive. Visualizations of the clusters showed clear groupings of data points, although some points were located between clusters, suggesting that the clusters could be further refined. The K-Means algorithm is sensitive to the initial placement of centroids, which can affect the final clusters. This limitation can be mitigated by running the algorithm multiple times with different initializations (using techniques like K-Means++ to improve the initialization process).\nDespite its strengths, K-Means has limitations. It assumes that clusters are spherical and equally sized, which may not be true for all datasets. Additionally, it requires specifying the number of clusters (K) in advance, which can be challenging when the optimal number is not obvious. In this case, the elbow method and silhouette scores helped determine the best K, but this is not always straightforward in other scenarios. K-Means also struggles with outliers, as they can disproportionately influence the position of centroids. However, when the data is well-suited to K-Means, as seen in our analysis, it provides a powerful and efficient clustering method.\n\n#from sklearn.cluster import DBSCAN\n#from sklearn.metrics import silhouette_score\n#import numpy as np\n\n# def dbscan_grid_search(data, eps_range, min_samples_range):\n#     best_score = -1\n#     best_params = {'eps': None, 'min_samples': None}\n    \n#     for eps in eps_range:\n#         for min_samples in min_samples_range:\n#             # Apply DBSCAN\n#             dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n#             labels = dbscan.fit_predict(data)\n            \n#             # DBSCAN labels: -1 represents noise, so we need at least 2 clusters to compute silhouette score\n#             if len(set(labels)) &gt; 1:\n#                 score = silhouette_score(data, labels)\n#                 print(f\"eps: {eps}, min_samples: {min_samples}, Silhouette Score: {score:.4f}\")\n#                 if score &gt; best_score:\n#                     best_score = score\n#                     best_params = {'eps': eps, 'min_samples': min_samples}\n    \n#     print(f\"Best Parameters: {best_params}, Best Silhouette Score: {best_score:.4f}\")\n#     return best_params, best_score\n\n# # Example usage:\n# eps_range = np.linspace(0.1, 1.0, 10)  # Modify the range as needed\n# min_samples_range = range(3, 10)  # Modify the range as needed\n# best_params, best_score = dbscan_grid_search(unsupervised_learner.data, eps_range, min_samples_range)\n\n#Best Parameters: {'eps': 0.1, 'min_samples': 5}, Best Silhouette Score: 0.5665\n\nunsupervised_learner.dbscan_clustering(eps=0.1, min_samples=5)\n\nDBSCAN Silhouette Score: 0.5664975908757899"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#dbscan",
    "href": "technical-details/unsupervised-learning/main.html#dbscan",
    "title": "Unsupervised Learning",
    "section": "DBSCAN",
    "text": "DBSCAN\nDBSCAN is a density-based clustering algorithm that groups points that are closely packed together, marking as noise points that lie in low-density regions. Unlike K-Means, DBSCAN does not require the user to specify the number of clusters in advance. Instead, it relies on two key parameters: epsilon (eps), which defines the maximum distance between two points for them to be considered neighbors, and min_samples, which specifies the minimum number of neighbors required to form a dense region. DBSCAN’s ability to detect noise points makes it especially useful in real-world datasets that contain outliers or irregular cluster shapes.\nIn our experiment, DBSCAN was applied with a grid search to find the optimal parameters. The best combination was found to be eps = 0.1 and min_samples = 5. However, the algorithm resulted in a silhouette score of 0.5665, which indicates moderate cluster cohesion and separation. Additionally, many points were labeled as noise (indicated by a label of -1), meaning they did not belong to any cluster. This suggests that DBSCAN struggled to find meaningful clusters in this dataset, possibly due to the relatively uniform distribution of points or the choice of parameters.\nOne of DBSCAN’s main strengths is its ability to detect arbitrarily shaped clusters and identify outliers as noise, which is useful for datasets with complex structures. However, DBSCAN is highly sensitive to the choice of parameters, especially eps. If eps is too small, the algorithm may label too many points as noise, while if eps is too large, it may merge distinct clusters. Additionally, DBSCAN can struggle when clusters have varying densities, as it uses the same density criteria for all clusters. In our analysis, the inability to form meaningful clusters and the high number of noise points suggest that DBSCAN is not the best choice for this dataset.\n\n# from sklearn.metrics import silhouette_score\n# from sklearn.cluster import AgglomerativeClustering\n# import numpy as np\n\n# def hierarchical_grid_search(data, method_range, distance_threshold_range):\n#     best_score = -1\n#     best_params = {'method': None, 'distance_threshold': None}\n    \n#     for method in method_range:\n#         for distance_threshold in distance_threshold_range:\n#             # Apply Agglomerative Clustering\n#             hierarchical = AgglomerativeClustering(linkage=method, distance_threshold=distance_threshold, n_clusters=None)\n#             labels = hierarchical.fit_predict(data)\n            \n#             # Get number of unique clusters\n#             unique_labels = set(labels)\n#             num_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)  # Exclude noise (-1) if present\n\n#             # Check if the number of clusters is valid for silhouette score (between 2 and n_samples-1)\n#             if num_clusters &gt;= 2 and num_clusters &lt; len(data):  # Ensure at least two clusters\n#                 try:\n#                     score = silhouette_score(data, labels)\n#                     print(f\"method: {method}, distance_threshold: {distance_threshold}, Silhouette Score: {score:.4f}\")\n                    \n#                     if score &gt; best_score:\n#                         best_score = score\n#                         best_params = {'method': method, 'distance_threshold': distance_threshold}\n#                 except ValueError:\n#                     print(f\"Error calculating silhouette score for method: {method}, distance_threshold: {distance_threshold}\")\n#             else:\n#                 print(f\"Skipping method: {method}, distance_threshold: {distance_threshold} due to invalid number of clusters.\")\n    \n#     print(f\"Best Parameters: {best_params}, Best Silhouette Score: {best_score:.4f}\")\n#     return best_params, best_score\n\n# # Example usage:\n# method_range = ['ward', 'complete', 'average', 'single']  # Different linkage methods\n# distance_threshold_range = np.linspace(2, 10000, 10)  # Modify the range as needed\n\n# best_params, best_score = hierarchical_grid_search(unsupervised_learner.data, method_range, distance_threshold_range)\n#Best Parameters: {'method': 'single', 'distance_threshold': 10000.0}, Best Silhouette Score: 0.4168\n\nunsupervised_learner.hierarchical_clustering(\"single\",distance_threshold = 10000)\n\n\n\n\n\n\n\n\nHierarchical Clustering Silhouette Score: 0.4168"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#hierarchical-clustering",
    "href": "technical-details/unsupervised-learning/main.html#hierarchical-clustering",
    "title": "Unsupervised Learning",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical clustering is a method of cluster analysis that builds a hierarchy of clusters. It can be performed using either an agglomerative approach (bottom-up) or a divisive approach (top-down). In agglomerative hierarchical clustering, each data point starts as its own cluster, and pairs of clusters are merged at each step based on their similarity, typically measured using a distance metric (e.g., Euclidean distance). The process continues until all points are merged into a single cluster or until a predefined stopping criterion is met. The result of hierarchical clustering is typically represented in a dendrogram, which shows the hierarchical relationships between the clusters.\nIn our analysis, we applied agglomerative hierarchical clustering with various methods (single, complete, average) and distance thresholds. The single linkage method, which merges clusters based on the closest pair of points, resulted in the best silhouette score of 0.4168. The clusters formed were relatively coherent, but the silhouette score indicates that the clustering was less distinct compared to K-Means. Hierarchical clustering’s main advantage is that it does not require the number of clusters to be specified in advance, as the tree-like structure allows for flexible cluster cutting at any level. However, the resulting clusters may not always be as well-separated as those produced by K-Means, especially when the data has more complex structures.\nWhile hierarchical clustering provides useful insights into the relationships between data points, it can become computationally expensive with large datasets due to its pairwise distance calculations. The method also tends to be sensitive to the choice of distance metric and linkage method. In this case, the single method and distance threshold of 10000.0 gave the best results, but the moderate silhouette score suggests that the clusters were not as well-defined as with K-Means. This makes hierarchical clustering more suitable for exploratory data analysis or when hierarchical relationships are important, rather than when distinct and well-separated clusters are required for practical applications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#results",
    "href": "technical-details/unsupervised-learning/main.html#results",
    "title": "Unsupervised Learning",
    "section": "Results",
    "text": "Results\nIn this section, we present and analyze the outcomes from the different clustering methods applied to the dataset. We evaluated the effectiveness of K-Means, DBSCAN, and Hierarchical Clustering, highlighting the strengths and weaknesses of each approach.\nK-Means Clustering:\n•   We applied K-Means clustering with an optimal number of clusters identified through the elbow method. The silhouette score of 0.8908 indicates strong cluster cohesion and separation. The clusters were well-defined when visualized on the PCA-reduced data. However, there were some ambiguous points between clusters 0 and 1, suggesting that further refinement could improve the classification.\nDBSCAN Clustering:\n•   DBSCAN, with the optimal parameters (eps: 0.1, min_samples: 5), resulted in a silhouette score of 0.5665. Several points were labeled as noise, indicating that DBSCAN struggled to form meaningful clusters. This suggests that DBSCAN may not be suitable for this dataset, as it does not capture the inherent structure as well as K-Means.\nHierarchical Clustering:\n•   Hierarchical clustering with the single method and a distance_threshold of 10000.0 achieved a silhouette score of 0.4168. This result suggests that the method produced somewhat coherent clusters, but with less clarity than K-Means. The dendrogram helped visualize the relationships between samples, but the moderate silhouette score indicates the clusters were not highly distinct.\nCluster Visualization and Comparison:\n•   Visualizations were used to display the results of each clustering method, highlighting the separation of clusters. PCA and t-SNE reduction techniques were employed to project the data into 2D, making it easier to visualize the clustering outcomes.\n\n•   K-Means clusters appeared well-separated, though there were some overlapping points.\n\n•   DBSCAN showed scattered clusters with many noise points.\n\n•   Hierarchical clustering, while offering a reasonable structure, did not achieve the same level of separation as K-Means.\nCluster-Label Comparison:\n•   When comparing the clustering results with the known labels in the dataset, we observed that the K-Means clustering provided the best match in terms of consistency. DBSCAN and Hierarchical clustering, however, did not align as well, especially with DBSCAN’s high number of noise points."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#conclusion",
    "href": "technical-details/unsupervised-learning/main.html#conclusion",
    "title": "Unsupervised Learning",
    "section": "Conclusion",
    "text": "Conclusion\nIn this analysis, we applied three clustering techniques to understand the underlying structure of the dataset: K-Means, DBSCAN, and Hierarchical Clustering.\nKey Findings:\n•   K-Means clustering emerged as the most effective method, yielding clear clusters with strong cohesion and separation, as reflected in its high silhouette score of 0.8908.\n\n•   DBSCAN, while offering the advantage of identifying noise points, did not perform well on this dataset, with a lower silhouette score (0.5665) and many points classified as noise.\n\n•   Hierarchical clustering, despite providing some useful insights into the data structure, resulted in a moderate silhouette score (0.4168), indicating that its performance was not as robust as K-Means.\nPractical Implications:\n•   K-Means is the most suitable clustering technique for this dataset, offering clear, actionable clusters that can inform further analysis or decision-making processes.\n\n•   DBSCAN’s inability to form meaningful clusters suggests that it may not be the right choice for datasets with complex, but dense structures.\n\n•   Hierarchical clustering provides valuable hierarchical relationships but may not be the best method when distinct cluster separation is needed for practical applications.\nThese findings suggest that K-Means could be used for segmenting this type of data in real-world applications such as food waste analysis, where identifying distinct clusters of waste categories or nutritional groups is important for developing targeted sustainability measures."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#introduction-and-motivation",
    "href": "technical-details/unsupervised-learning/main.html#introduction-and-motivation",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "In the context of this project, unsupervised learning will help identify groups of food items with similar waste characteristics, such as nutritional composition, environmental impact, and economic loss. These clusters can provide actionable insights for targeting interventions, whether through donation strategies, better supply chain management, or more efficient waste disposal practices. Additionally, dimensionality reduction methods like PCA and t-SNE will be used to visualize the multidimensional nature of the food waste data, offering a more intuitive understanding of how different factors correlate and contribute to the problem. By applying unsupervised learning techniques to the Nutrient Waste dataset, this project aims to uncover novel insights that can inform more sustainable food management strategies and policies."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#introduction-and-motivation",
    "href": "technical-details/supervised-learning/main.html#introduction-and-motivation",
    "title": "Supervised Learning",
    "section": "",
    "text": "While unsupervised learning helps uncover patterns in food waste data, supervised learning can provide actionable predictions that directly inform decision-making processes. Supervised learning techniques, such as classification and regression, are essential for predicting outcomes like the surplus dollar value of wasted food or classifying food donations based on nutritional composition. By training models on labeled data, we can predict which food items are most likely to be wasted, which can help inform better resource allocation and waste reduction strategies.\nIn this project, supervised learning will be used to predict the economic impact of food waste by estimating the dollar surplus associated with surplus food. This involves building regression models that take into account factors such as the nutritional content, environmental footprint, and food category to forecast economic losses. Additionally, classification models will be employed to identify which food donations are most suitable for redistribution based on their nutritional value and waste characteristics. These insights will guide the development of policies and practices to reduce food waste and its associated environmental and economic costs. By combining supervised learning with unsupervised insights, this project aims to create a comprehensive framework for mitigating food waste through data-driven strategies."
  },
  {
    "objectID": "technical-details/data-collection/methods.html#exploratory-data-analysis-eda",
    "href": "technical-details/data-collection/methods.html#exploratory-data-analysis-eda",
    "title": "Methods",
    "section": "",
    "text": "• Data cleaning includes handling missing values, standardizing nutrient columns to consistent units (e.g., grams or milligrams per serving), and addressing outliers.\n• Visualization of key metrics like food surplus, waste, and nutrient content over time is used to identify patterns."
  },
  {
    "objectID": "technical-details/data-collection/methods.html#standardization-of-nutrient-columns",
    "href": "technical-details/data-collection/methods.html#standardization-of-nutrient-columns",
    "title": "Methods",
    "section": "",
    "text": "Nutritional data for protein, fat, carbs, fiber, vitamins, and minerals were often expressed in inconsistent units across the dataset (e.g., grams for macronutrients and milligrams for micronutrients). Standardizing these units enables meaningful comparisons across categories. For example:\n• Macronutrients (protein, fat, carbs) are standardized to grams per serving.\n• Micronutrients (iron, calcium, potassium) are converted to milligrams per serving."
  },
  {
    "objectID": "technical-details/data-collection/methods.html#statistical-summaries-and-relationships",
    "href": "technical-details/data-collection/methods.html#statistical-summaries-and-relationships",
    "title": "Methods",
    "section": "",
    "text": "• Correlation analysis is used to identify links between food waste and nutrient density.\n• Regression models explore relationships between waste generation and external factors like supply and surplus."
  },
  {
    "objectID": "technical-details/data-collection/methods.html#clustering-nutritional-profiles",
    "href": "technical-details/data-collection/methods.html#clustering-nutritional-profiles",
    "title": "Methods",
    "section": "",
    "text": "• Techniques like K-Means are applied to group food categories based on nutritional similarity, highlighting waste trends in nutrient-dense versus nutrient-poor foods.\nThese methods provide a robust framework for understanding food waste and its nutritional implications while offering actionable insights."
  },
  {
    "objectID": "technical-details/data-collection/methods.html#references",
    "href": "technical-details/data-collection/methods.html#references",
    "title": "Methods",
    "section": "",
    "text": "• Everitt, B., Landau, S., Leese, M., & Stahl, D. (2011). Cluster Analysis. John Wiley & Sons.\n• Montgomery, D. C., Peck, E. A., & Vining, G. G. (2012). Introduction to Linear Regression Analysis. Wiley."
  },
  {
    "objectID": "technical-details/data-collection/main.html#references",
    "href": "technical-details/data-collection/main.html#references",
    "title": "Data Collection",
    "section": "References:",
    "text": "References:\n• Everitt, B., Landau, S., Leese, M., & Stahl, D. (2011). Cluster Analysis. John Wiley & Sons.\n• Montgomery, D. C., Peck, E. A., & Vining, G. G. (2012). Introduction to Linear Regression Analysis. Wiley."
  },
  {
    "objectID": "technical-details/data-collection/main.html#exploratory-data-analysis-eda",
    "href": "technical-details/data-collection/main.html#exploratory-data-analysis-eda",
    "title": "Data Collection",
    "section": "1. Exploratory Data Analysis (EDA):",
    "text": "1. Exploratory Data Analysis (EDA):\n• Data cleaning includes handling missing values, standardizing nutrient columns to consistent units (e.g., grams or milligrams per serving), and addressing outliers.\n• Visualization of key metrics like food surplus, waste, and nutrient content over time is used to identify patterns."
  },
  {
    "objectID": "technical-details/data-collection/main.html#standardization-of-nutrient-columns",
    "href": "technical-details/data-collection/main.html#standardization-of-nutrient-columns",
    "title": "Data Collection",
    "section": "2. Standardization of Nutrient Columns:",
    "text": "2. Standardization of Nutrient Columns:\nNutritional data for protein, fat, carbs, fiber, vitamins, and minerals were often expressed in inconsistent units across the dataset (e.g., grams for macronutrients and milligrams for micronutrients). Standardizing these units enables meaningful comparisons across categories. For example:\n• Macronutrients (protein, fat, carbs) are standardized to grams per serving.\n• Micronutrients (iron, calcium, potassium) are converted to milligrams per serving."
  },
  {
    "objectID": "technical-details/data-collection/main.html#statistical-summaries-and-relationships",
    "href": "technical-details/data-collection/main.html#statistical-summaries-and-relationships",
    "title": "Data Collection",
    "section": "3. Statistical Summaries and Relationships:",
    "text": "3. Statistical Summaries and Relationships:\n• Correlation analysis is used to identify links between food waste and nutrient density.\n• Regression models explore relationships between waste generation and external factors like supply and surplus."
  },
  {
    "objectID": "technical-details/data-collection/main.html#clustering-nutritional-profiles",
    "href": "technical-details/data-collection/main.html#clustering-nutritional-profiles",
    "title": "Data Collection",
    "section": "4. Clustering Nutritional Profiles:",
    "text": "4. Clustering Nutritional Profiles:\n• Techniques like K-Means are applied to group food categories based on nutritional similarity, highlighting waste trends in nutrient-dense versus nutrient-poor foods.\nThese methods provide a robust framework for understanding food waste and its nutritional implications while offering actionable insights."
  },
  {
    "objectID": "technical-details/data-collection/main.html#references-1",
    "href": "technical-details/data-collection/main.html#references-1",
    "title": "Data Collection",
    "section": "References:",
    "text": "References:\n• Everitt, B., Landau, S., Leese, M., & Stahl, D. (2011). Cluster Analysis. John Wiley & Sons.\n• Montgomery, D. C., Peck, E. A., & Vining, G. G. (2012). Introduction to Linear Regression Analysis. Wiley."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-source-information",
    "href": "technical-details/data-collection/main.html#data-source-information",
    "title": "Data Collection",
    "section": "Data Source Information",
    "text": "Data Source Information\n• Food Waste Data: The food waste data comes from the ReFED Food Surplus Database, which is publicly available. It includes detailed food waste information, such as the category of food waste, food type, and surplus amounts. The data can be accessed from the following link.\n• Nutrient Data: Nutrient data is fetched using the USDA Food Database API. The API provides detailed information on food products, including nutrient content (e.g., calories, protein, fat, etc.), serving size, food category, and other attributes. The API documentation can be found here.\n• Original Data: The IPNI Estimates of Nutrient Uptake and Removal dataset is extracted from a PDF document using the pdfplumber library. The extracted data is then processed and saved as CSV files for further analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-collection-methods",
    "href": "technical-details/data-collection/main.html#data-collection-methods",
    "title": "Data Collection",
    "section": "Data Collection Methods",
    "text": "Data Collection Methods\n• API Use: The USDA Food Database API is used to gather detailed nutritional information for various food items. This includes attributes such as food name, serving size, brand, and various nutrients (protein, fat, carbs, vitamins, minerals, etc.). A Python script fetches data for each unique food name from the dataset food_waste. The API request is structured with parameters like food name and page size to collect relevant information."
  },
  {
    "objectID": "technical-details/data-collection/main.html#relevance-to-the-project",
    "href": "technical-details/data-collection/main.html#relevance-to-the-project",
    "title": "Data Collection",
    "section": "Relevance to the Project",
    "text": "Relevance to the Project\nThe ReFED Food Surplus Data is key to understanding food waste patterns across different food categories, helping identify the impact of food waste on the environment and the economy. The USDA Food Database is essential for providing the nutritional content of food items, enabling a comprehensive analysis of the relationship between food waste and nutrition. The IPNI Nutrient Data further supports the project by offering estimates of nutrient uptake and removal, which can be linked to food surplus and waste. These datasets together help analyze food waste, nutrients, and the overall environmental and health impact of wasted food."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#introduction-and-motivation",
    "href": "technical-details/data-collection/overview.html#introduction-and-motivation",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Food waste is a global issue with profound economic, environmental, and social implications. According to the Food and Agriculture Organization (FAO), roughly one-third of all food produced globally—about 1.3 billion tons—is wasted annually, leading to significant losses in natural resources, energy, and human labor (FAO, 2013). In the United States, the Environmental Protection Agency (EPA) estimates that food waste constitutes 24% of municipal solid waste, making it the single largest category in landfills (EPA, 2021).\nThis analysis examines a comprehensive dataset capturing the surplus and waste of various food categories, along with their nutritional compositions. The goal is to uncover insights into:\n1.  The patterns and drivers of food surplus and waste.\n\n2.  The relationship between nutrient density and food waste across categories.\n\n3.  How resources like water and energy are impacted by food wastage.\nBy quantifying these metrics and exploring trends, this analysis aims to inform policies and interventions to mitigate food waste while addressing nutritional and environmental challenges."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#references",
    "href": "technical-details/data-collection/overview.html#references",
    "title": "Overview",
    "section": "",
    "text": "• Food and Agriculture Organization (FAO). (2013). Food Wastage Footprint: Impacts on Natural Resources. Retrieved from FAO.org.\n• Environmental Protection Agency (EPA). (2021). Food Recovery Hierarchy. Retrieved from EPA.gov."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#managing-missing-data",
    "href": "technical-details/data-cleaning/main.html#managing-missing-data",
    "title": "Data Cleaning",
    "section": "Managing Missing Data",
    "text": "Managing Missing Data\n• Identify Missing Values: Missing values are common in real-world datasets and can occur for various reasons. In this project, missing data was identified in columns such as serving_size, protein, fat, carbs, calories, and other nutritional values. Using pandas’ isnull() function, the locations of missing values were pinpointed across the dataset.\n• Handling Missing Data: Several strategies were employed to handle missing data. For numerical columns like protein, fat, and calories, missing values were replaced with the mean of the column to retain the data’s overall distribution. In some cases, rows with missing critical data, such as serving_size, were dropped entirely to ensure the integrity of further analysis."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#outlier-detection-and-treatment",
    "href": "technical-details/data-cleaning/main.html#outlier-detection-and-treatment",
    "title": "Data Cleaning",
    "section": "Outlier Detection and Treatment",
    "text": "Outlier Detection and Treatment\n• Identify Outliers: Outliers were detected using statistical methods such as Z-scores and box plots. Variables like serving_size, protein, fat, and other nutritional values were checked for extreme values that might indicate outliers.\n• Addressing Outliers: Outliers were addressed depending on their nature. In some cases, extreme values were capped or transformed, while in others, they were removed if they were clearly erroneous. Retaining outliers for analysis was also considered when they provided valuable insights into the data distribution.\n• Visualize Outliers: Box plots were used to visualize the outliers before and after handling. These plots illustrated the distribution of values and how outliers were managed in various columns. By comparing the before and after visualizations, the effect of outlier treatment on the dataset could be clearly seen."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#normalization-and-scaling",
    "href": "technical-details/data-cleaning/main.html#normalization-and-scaling",
    "title": "Data Cleaning",
    "section": "Normalization and Scaling:",
    "text": "Normalization and Scaling:\n• Data Distribution Analysis: The distribution of numerical variables was initially analyzed using histograms and summary statistics to identify any skewness or irregular distributions. For example, variables like protein, carbs, and fat exhibited skewed distributions.\n• Normalization Techniques: To address this, normalization techniques such as min-max scaling and Z-score normalization were applied to certain columns. These methods ensured that variables with different ranges could be compared directly and improved the performance of certain machine learning models.\n• Before-and-After Visualizations: Before and after visualizations, such as histograms and box plots, were used to compare the data distributions. The effect of normalization could be seen in the flattening of skewed distributions, making them more suitable for modeling."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#subsetting-the-data",
    "href": "technical-details/data-cleaning/main.html#subsetting-the-data",
    "title": "Data Cleaning",
    "section": "Subsetting the Data",
    "text": "Subsetting the Data\n• Data Filtering: The data was subsetted to focus on relevant columns and remove irrelevant ones. For example, columns related to allergens, microbes, and country of origin were removed, as they were not directly relevant to the analysis of food waste and nutritional content. Filtering also involved focusing on specific food categories and nutritional attributes, such as protein, fat, calories, and fiber.\n• Rationale: The rationale for filtering was to reduce the complexity of the data and focus on the most relevant features for analysis. By working with a smaller, more targeted subset, it was easier to perform focused analysis and avoid noise from unnecessary variables. The subsetted data was better suited for modeling and understanding the relationships between food waste and nutritional content."
  },
  {
    "objectID": "technical-details/data-collection/main.html#introduction-and-motivation",
    "href": "technical-details/data-collection/main.html#introduction-and-motivation",
    "title": "Data Collection",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nFood waste is a global issue with profound economic, environmental, and social implications. According to the Food and Agriculture Organization (FAO), roughly one-third of all food produced globally—about 1.3 billion tons—is wasted annually, leading to significant losses in natural resources, energy, and human labor (FAO, 2013). In the United States, the Environmental Protection Agency (EPA) estimates that food waste constitutes 24% of municipal solid waste, making it the single largest category in landfills (EPA, 2021).\nThis analysis examines a comprehensive dataset capturing the surplus and waste of various food categories, along with their nutritional compositions. The goal is to uncover insights into:\n1.  The patterns and drivers of food surplus and waste.\n\n2.  The relationship between nutrient density and food waste across categories.\n\n3.  How resources like water and energy are impacted by food wastage.\nBy quantifying these metrics and exploring trends, this analysis aims to inform policies and interventions to mitigate food waste while addressing nutritional and environmental challenges."
  },
  {
    "objectID": "technical-details/data-collection/main.html#code",
    "href": "technical-details/data-collection/main.html#code",
    "title": "Data Collection",
    "section": "Code",
    "text": "Code\n\nimport requests\nimport pandas as pd\nimport json\nimport fitz  \nimport pdfplumber\nfrom rapidfuzz import process\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport pandas as pd\nfrom difflib import SequenceMatcher\n\n\nwith open('technical-details/data-collection/config.json') as f:\n    keys = json.load(f)\nAPI_KEY = keys['fdaapi']\n\nBASE_URL = \"https://api.nal.usda.gov/fdc/v1/foods/search\"\n\n#SOURE : https://insights-engine.refed.org/food-waste-monitor?break_by=destination&indicator=tons-surplus&view=detail&year=2018\nfoodwaste = pd.read_csv(\"data/raw-data/ReFED_US_Food_Surplus_Detail.csv\")\nfoodwaste.columns\nfoodwaste[\"food_category\"].nunique()\n\nfoodwaste['food_name'] = foodwaste['food_category'].where(foodwaste['food_category'] != \"Not Applicable\", foodwaste['food_type'])\n\ndef fetch_food_data(df, page_size=12):\n    all_results = [] \n    if 'food_name' not in df.columns or df['food_name'].empty:\n        raise ValueError(\"DataFrame must contain a non-empty 'food_name' column\")\n\n    for food_name in df['food_name'].unique():\n        params = {\n            'query': str(food_name),\n            'api_key': API_KEY,\n            'pageSize': page_size\n        }\n\n        response = requests.get(BASE_URL, params=params)\n\n        if response.status_code == 200:\n            results = response.json()\n            if 'foods' in results:\n                all_results.extend(results['foods'])\n        else:\n            print(f\"Error for {food_name}: {response.status_code}\")\n\n    return all_results\n\n\ndef process_food_data(data):\n    food_items = []\n\n    if data:\n        for food in data:\n            if 'servingSizeUnit' in food and food['servingSizeUnit']:\n                food_info = {\n                    'food_name': food.get('description', ''),\n                    'fdc_id': food.get('fdcId', ''),\n                    'brand': food.get('brandOwner', ''),\n                    'food_category': food.get('foodCategory', ''),\n                    'market_country': food.get('marketCountry', ''),\n                    'serving_size': food.get('servingSize', 0),\n                    'serving_size_unit': food.get('servingSizeUnit', ''),\n                }\n\n                for nutrient in food.get('foodNutrients', []):\n                    nutrient_name = nutrient.get('nutrientName', '')\n                    nutrient_value = nutrient.get('value', 0)\n\n                    if nutrient_name == 'Energy':  # Calories\n                        food_info['calories'] = nutrient_value\n                    elif nutrient_name == 'Protein':\n                        food_info['protein'] = nutrient_value\n                    elif nutrient_name == 'Total lipid (fat)':  # Fat\n                        food_info['fat'] = nutrient_value\n                    elif nutrient_name == 'Carbohydrate, by difference':  # Carbs\n                        food_info['carbs'] = nutrient_value\n                    elif nutrient_name == 'Fiber, total dietary':  # Fiber\n                        food_info['fiber'] = nutrient_value\n                    elif nutrient_name == 'Sugars, total':  # Sugar\n                        food_info['sugar'] = nutrient_value\n                    elif nutrient_name == 'Vitamin A, IU':  # Vitamin A (IU)\n                        food_info['vitamin_a_iu'] = nutrient_value\n                    elif nutrient_name == 'Vitamin C, total ascorbic acid':  # Vitamin C (mg)\n                        food_info['vitamin_c_mg'] = nutrient_value\n                    elif nutrient_name == 'Cholesterol':  # Cholesterol (mg)\n                        food_info['cholesterol_mg'] = nutrient_value\n                    elif nutrient_name == 'Fatty acids, total saturated':  # Saturated Fat (g)\n                        food_info['saturated_fat_g'] = nutrient_value\n                    elif nutrient_name == 'Calcium, Ca':  # Calcium (mg)\n                        food_info['calcium'] = nutrient_value\n                    elif nutrient_name == 'Iron, Fe':  # Iron (mg)\n                        food_info['iron'] = nutrient_value\n                    elif nutrient_name == 'Sodium, Na':  # Sodium (mg)\n                        food_info['sodium'] = nutrient_value\n                    elif nutrient_name == 'Potassium, K':  # Potassium (mg)\n                        food_info['potassium'] = nutrient_value\n                    elif nutrient_name == 'Magnesium, Mg':  # Magnesium (mg)\n                        food_info['magnesium'] = nutrient_value\n                    elif nutrient_name == 'Phosphorus, P':  # Phosphorus (mg)\n                        food_info['phosphorus'] = nutrient_value\n\n                    food_info['percent_daily_value'] = nutrient.get('percentDailyValue', 0)\n\n                food_info['microbes'] = ', '.join(str(microbe) for microbe in food.get('microbes', [])) if food.get('microbes') else 'No microbes listed'\n                \n                food_info['allergens'] = ', '.join(str(allergen) for allergen in food.get('allergens', ['None'])) if food.get('allergens') else 'None'\n\n                food_info['additives'] = ', '.join(str(additive) for additive in food.get('additives', [])) if food.get('additives') else 'None'\n\n                food_info['labels'] = ', '.join(food.get('labels', ['None'])) if food.get('labels') else 'None'\n\n                food_info['nutrient_group'] = food.get('nutrientGroup', 'Unknown')\n                food_info['brand_name'] = food.get('brandOwner', 'Unknown')\n                food_info['food_labels'] = ', '.join(food.get('labels', ['None'])) if food.get('labels') else 'None'\n                food_info['package_size'] = food.get('packageSize', 'Not available')\n                food_info['food_safety_info'] = food.get('foodSafetyInfo', 'No safety info available')\n                food_info['expiration_date'] = food.get('expirationDate', 'Not available')\n                food_info['country_of_origin'] = food.get('countryOfOrigin', 'Not specified')\n\n                food_items.append(food_info)\n\n    if food_items:\n        food_df = pd.DataFrame(food_items)\n    else:\n        food_df = pd.DataFrame()\n\n    return food_df\n\nfood_data = foodwaste(foodwaste)\n\n\n\nfood_data.head()\nfood_data.shape\nfoodwaste.head()\nfoodwaste.shape\n\nprint(food_data)\nprint(foodwaste)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#code",
    "href": "technical-details/data-cleaning/main.html#code",
    "title": "Data Cleaning",
    "section": "Code",
    "text": "Code\n\nimport requests\nimport pandas as pd\nimport json\nimport fitz  \nimport pdfplumber\nfrom rapidfuzz import process\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport pandas as pd\nfrom difflib import SequenceMatcher\n\nfood_data = pd.read_csv(\"data/raw-data/foods_data.csv\")\nfood_waste = pd.read_csv(\"data/raw-data/ReFED_US_Food_Surplus_Detail.csv\")\n\nfood_waste.drop(\n    ['Unnamed: 0','sector', 'sub_sector', 'sub_sector_category',\n       'food_type','surplus_upstream_100_year_mtco2e_footprint',\n     'surplus_downstream_100_year_mtco2e_footprint',\n     'surplus_total_100_year_mtco2e_footprint',\n     'surplus_upstream_100_year_mtch4_footprint',\n     'surplus_downstream_100_year_mtch4_footprint',\n     'surplus_total_100_year_mtch4_footprint'], \n    axis=1, \n    inplace=True\n)\n\nfood_data.drop(\n    ['microbes', 'allergens', 'additives', 'labels',\n       'nutrient_group', 'brand_name', 'food_labels', 'package_size',\n       'food_safety_info', 'expiration_date', 'country_of_origin','fdc_id' ],\n    axis=1, \n    inplace=True\n)\n\n\n\nvalid_units = ['g', 'grm', 'gm', 'mlt', 'ml', 'mg']\n\n\ndf_filtered = food_data[food_data['serving_size_unit'].str.lower().isin(valid_units)]\n\ndf_filtered.columns\n\ndef convert_to_grams(row, column):\n    unit = row['serving_size_unit'].lower()\n    \n    # If unit is grams (g, grm, gm), return the value as is\n    if unit in ['g', 'grm', 'gm']:\n        return row[column]\n    # If unit is milliliters (ml, mlt), assume 1 ml = 1 g (for liquids)\n    elif unit in ['ml', 'mlt']:\n        return row[column]  # Convert 1 ml to 1 g\n    # If unit is milligrams (mg), convert to grams\n    elif unit == 'mg':\n        return row[column] * 0.001  # 1 mg = 0.001 g\n    # If unit is unknown or unsupported, return the value as is\n    else:\n        return row[column]\n\n# List of columns to convert\ncolumns_to_convert = [\n    'serving_size', 'protein', 'percent_daily_value', 'fat', 'carbs', 'calories', \n    'fiber'\n]\n\n# Apply conversion to each specified column\nfor column in columns_to_convert:\n    df_filtered[column] = df_filtered.apply(lambda row: convert_to_grams(row, column), axis=1)\n\ndef convert_to_grams(row):\n    # Convert Vitamin A (IU to grams)\n    vitamin_a_grams = row['vitamin_a_iu'] * 0.0000003 if pd.notnull(row['vitamin_a_iu']) else None\n    \n    # Convert Vitamin C (mg to grams)\n    vitamin_c_grams = row['vitamin_c_mg'] * 0.001 if pd.notnull(row['vitamin_c_mg']) else None\n    \n    # Convert Cholesterol (mg to grams)\n    cholesterol_grams = row['cholesterol_mg'] * 0.001 if pd.notnull(row['cholesterol_mg']) else None\n    \n    return pd.Series({'vitamin_a_grams': vitamin_a_grams, 'vitamin_c_grams': vitamin_c_grams, 'cholesterol_grams': cholesterol_grams})\n\n# Apply the conversion function to the DataFrame\ndf_filtered[['vitamin_a_grams', 'vitamin_c_grams', 'cholesterol_grams']] = df_filtered.apply(convert_to_grams, axis=1)\n\n# Drop the original columns\ndf_filtered = df_filtered.drop(columns=['vitamin_a_iu', 'vitamin_c_mg', 'cholesterol_mg'])\n\ndf_filtered = df_filtered.drop(columns=['brand', 'food_category', 'market_country'])\n\ndf_filtered.columns\ndf_filtered.head()\n\ndef mean_excluding_nulls(series):\n    return series.dropna().mean()\n\n\ndf_grouped = df_filtered.groupby('food_name').agg({\n    'serving_size': mean_excluding_nulls,\n    'protein': mean_excluding_nulls,\n    'percent_daily_value': mean_excluding_nulls,\n    'fat': mean_excluding_nulls,\n    'carbs': mean_excluding_nulls,\n    'calories': mean_excluding_nulls,\n    'fiber': mean_excluding_nulls,\n    'calcium': mean_excluding_nulls,\n    'iron': mean_excluding_nulls,\n    'potassium': mean_excluding_nulls,\n    'sodium': mean_excluding_nulls,\n    'phosphorus': mean_excluding_nulls\n}).reset_index()\n\n\n\ndef calculate_overlap(str1, str2):\n    matcher = SequenceMatcher(None, str1.lower(), str2.lower())\n    return matcher.ratio()  \ndef find_best_match(food_name, df, used_matches):\n    best_match = None\n    highest_overlap = 0\n    best_index = -1\n    \n    \n    for idx, name2 in df['food_name'].items():  \n        if idx not in used_matches:\n            overlap = calculate_overlap(food_name, name2)\n            if overlap &gt; highest_overlap:\n                best_match = name2\n                highest_overlap = overlap\n                best_index = idx\n    \n    return best_match, highest_overlap, best_index\nmatches = []\nused_matches = set()\n\nfor name1 in food_waste['food_name']:\n    best_match, score, best_index = find_best_match(name1, df_grouped, used_matches)\n    if best_match and score &gt;= 0.60:  \n        matches.append({\n            'food_name_food_waste': name1, \n            'best_match': best_match, \n            'score': score\n        })\n        used_matches.add(best_index) \n\nmatches_df = pd.DataFrame(matches)\n\nfood_waste_renamed = food_waste.rename(columns={'food_name': 'food_name_food_waste'})\n\nresult = pd.merge(food_waste_renamed, matches_df, left_on='food_name_food_waste', right_on='food_name_food_waste', how='left')\n\n\nresult = pd.merge(result, df_grouped, left_on='best_match', right_on='food_name', how='left', suffixes=('_food_waste', '_grouped_df'))\n\nprint(result)\ncolumns_to_convert = ['calcium', 'iron', 'potassium', 'sodium', 'phosphorus']\n\n#convert to grams\nfor col in columns_to_convert:\n    if col in result.columns:\n        result[col] = result[col] / 1000\n\n#drop rows where serving_size is NA\nresult = result.dropna(subset=['serving_size'])\nresult.columns\nresult.describe()\n\nresult.to_csv('data/processed-data/food_merged.csv')"
  },
  {
    "objectID": "report/report.html#executive-summary",
    "href": "report/report.html#executive-summary",
    "title": "Final Report",
    "section": "",
    "text": "This report focuses on analyzing patterns of food waste and employs a data-driven approach to address two key objectives: reducing food wastage and minimizing surplus costs in US dollars. By leveraging insights into the macronutrient profiles of donated and recycled foods, the project aims to better understand the composition and potential uses of discarded food.\nTo achieve these goals, machine learning models are developed to predict the journey of food waste, providing actionable insights into its lifecycle. These models help identify opportunities to redirect surplus food toward donations or recycling pathways, rather than contributing to waste. The project also seeks to design and optimize new pipelines to mitigate food waste, transforming discarded nutrients into valuable resources that can benefit both communities and the environment."
  },
  {
    "objectID": "report/report.html#objective",
    "href": "report/report.html#objective",
    "title": "Final Report",
    "section": "",
    "text": "This project seeks to analyze patterns in food waste with the goal of reducing wastage by increasing food donations and recycling opportunities. By examining food wastage trends over time, the project aims to design a nutrient reuptake pipeline, repurposing discarded nutrients for better use. Through a data-driven approach, the study investigates the journey of food waste, classifying its final destination—such as composting, animal feed, or donations—based on nutrient profiles.\nAdditionally, the project leverages machine learning models to predict the US Dollar surplus value using features derived from upstream processes. This predictive capability not only highlights areas of inefficiency but also provides actionable insights to reduce food production surpluses, optimize resource allocation, and align production more closely with demand, contributing to sustainability and cost reduction efforts."
  },
  {
    "objectID": "report/report.html#key-findings",
    "href": "report/report.html#key-findings",
    "title": "Final Report",
    "section": "",
    "text": "The key findings of this study are presented below -\n\nUsing input features such as ‘gallons_water_footprint,’ ‘serving_size,’ ‘calories,’ ‘protein,’ ‘fat,’ ‘carbs,’ ‘fiber,’ ‘calcium,’ ‘iron,’ and ‘sodium,’ our Random Forest model achieves an impressive classification accuracy of 93%. This model identifies the primary disposal method for various food items, providing actionable insights into food waste management. The high accuracy of this model offers a practical tool for consumers, helping them determine which foods are suitable for recycling as compost or animal feed. By promoting informed disposal decisions, this approach can contribute to reducing overall food waste and supporting ssustainable waste management practices.\nBased on the aforementioned input features our KNN model can classify a food item as either “donatable” or not with a cross validation score of approximately 94%. Integrating this model into the existing logistics pipeline for food waste could help boost food donation efforts and spearhead more sustainable food management practices.\nUsing input features such as ‘serving_size,’ ‘calories,’ ‘protein,’ ‘fat,’ ‘carbs,’ ‘fiber,’ ‘calcium,’ ‘iron,’ ‘sodium,’ and ‘food_category’ (one-hot encoded), our Random Forest model achieved an R-squared value of approximately 0.968 in predicting the US Dollar Surplus value. This high R-squared value indicates that the model can accurately explain nearly 97% of the variability in surplus spending based on these input features."
  },
  {
    "objectID": "report/report.html#methodology-overview",
    "href": "report/report.html#methodology-overview",
    "title": "Final Report",
    "section": "",
    "text": "This analysis employed a combination of exploratory data analysis (EDA), normalization, and statistical modeling techniques to draw insights from the data. The following EDA techniques were used -\n• The data was cleaned so that missing values were handled, and nutrient columns were standardized and converted to their appropriate units (e.g., grams or milligrams per serving), and addressing outliers.\n• Visualizations were prepared to explore the relationships between the variables in the data.\nNutritional data for protein, fat, carbs, fiber, vitamins, and minerals were often expressed in inconsistent units across the dataset (e.g., grams for macronutrients and milligrams for micronutrients). Standardizing these units enabled meaningful comparisons across categories. For instance, macronutrients (protein, fat, carbs) were standardized to grams per serving.\nThen, the following supervised and unsupervised learning techniques were applied to the dataset -\n• Supervised Learning models like KNN, SVM and Randomforest were used for multiclass classification problems. These techniques were used to predict the primary disposal method for a certain food item and to predict whether a food item could be donated or not\n• Regression models were used to explore relationships between waste generation and external factors like supply and surplus.\n• Techniques like K-Means were applied to group food categories based on nutritional similarity, highlighting waste trends in nutrient-dense versus nutrient-poor foods.\nThese methods provided a robust framework for understanding food waste and its nutritional implications while offering actionable insights."
  },
  {
    "objectID": "report/report.html#visualizations",
    "href": "report/report.html#visualizations",
    "title": "Final Report",
    "section": "",
    "text": "This section provides a few important data visualizations along with drawn from them.\n\n\n\nServing Size (g) vs Tons Uneaten\n\n\nThe above plot helps visualize how serving sizes of food items (in grams) is positively correlated with tons of food uneaten. A major finding of this study is that smaller serving sizes can prevent food from being wasted. Furthermore, smaller serving sizes would also reduce the cost of production for companies in the long term.\n\n\n\nProtein Content (g) vs USD Surplus\n\n\nThe above plot indicates that high-protein foods are frequently associated with higher USD surplus values. This insight could be valuable in educating individuals and families about food consumption habits. Foods with high protein content disproportionately contribute to higher USD surplus values, highlighting their significant role in food waste and surplus production.\nTo mitigate this, consumers can be encouraged to plan their purchases more effectively, avoiding over-purchasing high-protein foods that are more likely to go unused. Additionally, this finding underscores the importance of targeted interventions in food donation or recycling programs, focusing on redistributing surplus high-protein foods to areas where they can be utilized, such as food banks or animal feed initiatives. By addressing the surplus generation of such nutrient-dense foods, efforts to reduce food waste can become more impactful and sustainable.\n\n\n\nProtein Content (g) vs Gallons of Water Footprint\n\n\nThe above plot demonstrates that foods with higher protein content (measured in grams) require significantly more gallons of water during upstream processing. This trend is likely attributable to high-protein foods such as beef, poultry, and dairy, which have a substantial environmental footprint due to intensive resource requirements in their production processes.\nThis finding emphasizes the need for promoting more sustainable dietary choices. Reducing the consumption of resource-intensive, high-protein foods can help minimize water usage and decrease the overall environmental impact of food production. Public awareness campaigns and policy interventions could encourage shifts toward plant-based proteins and other sustainable alternatives, fostering a more environmentally conscious approach to food consumption.\n\n\n\nFiber Content (g) vs Tons Recycled\n\n\nThe above plot illustrates that foods with a high fiber content, often less processed and more natural, are frequently recyclable and repurposable. This finding highlights the sustainability of consuming such foods, as they are less likely to contribute to waste and can instead serve as valuable inputs for recycling initiatives, such as composting or creating animal feed.\nEncouraging the consumption of high-fiber foods not only promotes healthier dietary habits but also aligns with sustainable food systems by reducing landfill contributions and supporting nutrient cycling. Public awareness campaigns and educational initiatives could emphasize the dual benefits of high-fiber foods—both for individual health and for reducing the environmental impact of food waste\n\n\n\nKernel Density Plot of Serving Size (g) vs USD Surplus\n\n\nThe above Kernel Density Plot highlights that foods with serving sizes between 30-80 grams or 150-200 grams contribute significantly to higher USD surplus values, indicating that these portions are more prone to being wasted. This insight underscores the importance of rethinking packaging strategies for these specific serving sizes.\nAdjusting packaging to better match consumption patterns—such as offering smaller, customizable portions or resealable options—could help mitigate waste. By aligning serving sizes more closely with consumer needs, the food industry can reduce excess production and waste, ultimately leading to both economic and environmental benefits."
  },
  {
    "objectID": "report/report.html#implications",
    "href": "report/report.html#implications",
    "title": "Final Report",
    "section": "",
    "text": "The implications of the finding from this report can be found below.\n\nAnticipating Excess Production Costs:\n\nThe models presented here, provide the food and beverage industry with a predictive tool to estimate surplus spending. By identifying patterns linked to food overproduction, food and beverage businesses can better plan production quantities, aligning supply more closely with demand.\n\nReducing Food Waste at the Source:\n\nBy forecasting surplus values, companies and individual consumers can take preemptive measures to reduce production of items prone to excessive waste. This can lead to a significant reduction in food surpluses, contributing to more sustainable food systems.\n\nCost Optimization:\n\nManufacturers and suppliers can leverage these predictions to reduce operational costs associated with surplus management, such as storage, distribution, or disposal expenses. This ultimately enhances profitability and operational efficiency.\n\nSupport for Sustainability Goals:\n\nWith actionable insights from the model, the food industry can make more environmentally responsible decisions by minimizing its water footprint stemming from surplus production and waste."
  },
  {
    "objectID": "report/report.html#recommendations",
    "href": "report/report.html#recommendations",
    "title": "Final Report",
    "section": "",
    "text": "Here are some of the recommendations from this study -\n\nOptimize food packaging - The packaging for foods with a serving size between 30-80 grams or 150-200 grams must be redesigned because they currently contribute disproportionately to USD surplus values. Resealable and portion-controlled packaging can help reduce waste from food.\nSupport for Recycling and Food Donation Infrastructure - Policy makers should invest in infrastructure for food recycling such as anaerobic digestion farms or composting units. Furthermore, there should be tax benefits for donating surplus food and a wider encouragement for food recovery iniatives.\nUtilizing Leftovers and Expired Foods - Individuals can incorporate leftover ingredients creatively to reduce waste and maximize food utilization.\nEmbracing High-Fiber Foods - Individual consumers should consider consuming foods with a high fiber content. Food waste which comes from fiber-rich foods should be composted and/or recycled.\nFurther Investigation into Nutrient Reuptake Systems - While this is a promising study, further research is required into the nutrient needs of different organisms so that discarded nutrients can be used for not just animal feed and anaerobic digestion but also as fodder for other organisms."
  },
  {
    "objectID": "report/report.html#conclusion",
    "href": "report/report.html#conclusion",
    "title": "Final Report",
    "section": "",
    "text": "In conclusion, this study is a promising step toward understanding and mitigating food waste by leveraging data-driven approaches. By analyzing the nutrient profiles and waste patterns of food, we have identified key factors that contribute to food surpluses and waste, such as serving size, protein content, and water footprint. The use of machine learning models has shown potential for predicting surplus values and identifying opportunities for food donations or recycling, thereby helping reduce food waste and its environmental impact. The recommendations provided can guide the food and beverage industry, policymakers, and consumers toward more sustainable practices, ultimately contributing to a more efficient and less wasteful food system. However, further research and continuous improvements in data collection and model accuracy will be essential to fully optimize these strategies for wider adoption."
  }
]